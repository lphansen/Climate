{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pricing Uncertainty Induced by Climate Change\n",
    "by [Michael Barnett](https://sites.google.com/site/michaelduglasbarnett/home), [William Brock](https://www.ssc.wisc.edu/~wbrock/) and [Lars Peter Hansen](https://larspeterhansen.org/).\n",
    "\n",
    "The latest draft of the paper can be found [here](https://larspeterhansen.org/research/papers/).\n",
    "\n",
    "Notebook by: Jiaming Wang\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides the source code and explanations for how we solve the model setting with __climate damages to growth__. The computational procedures are similar to the model setting with __climate damages to consumption__. Users should refer to the notebook for the [Consumption Damages Model](ConsumptionModel.ipynb) for more computational details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Overview</a></span></li><li><span><a href=\"#Code-and-Illustration\" data-toc-modified-id=\"Code-and-Illustration-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Code and Illustration</a></span><ul class=\"toc-item\"><li><span><a href=\"#Choosing-key-parameters\" data-toc-modified-id=\"Choosing-key-parameters-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Choosing key parameters</a></span></li><li><span><a href=\"#Solving-HJB-for-Growth-Damage\" data-toc-modified-id=\"Solving-HJB-for-Growth-Damage-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Solving HJB for Growth Damage</a></span><ul class=\"toc-item\"><li><span><a href=\"#Remark:--Accounting-for-uncertainty-about-growth-damages\" data-toc-modified-id=\"Remark:--Accounting-for-uncertainty-about-growth-damages-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Remark:  Accounting for uncertainty about growth damages</a></span><ul class=\"toc-item\"><li><span><a href=\"#Selecting-the-values-of-$\\gamma_1,-\\gamma_2$\" data-toc-modified-id=\"Selecting-the-values-of-$\\gamma_1,-\\gamma_2$-2.2.1.1\"><span class=\"toc-item-num\">2.2.1.1&nbsp;&nbsp;</span>Selecting the values of $\\gamma_1, \\gamma_2$</a></span></li><li><span><a href=\"#Varying-damage-models\" data-toc-modified-id=\"Varying-damage-models-2.2.1.2\"><span class=\"toc-item-num\">2.2.1.2&nbsp;&nbsp;</span>Varying damage models</a></span></li><li><span><a href=\"#Distorted-probabilities\" data-toc-modified-id=\"Distorted-probabilities-2.2.1.3\"><span class=\"toc-item-num\">2.2.1.3&nbsp;&nbsp;</span>Distorted probabilities</a></span></li></ul></li><li><span><a href=\"#Remark-Tolerance-level-and-conergence-criteria-for-HJB\" data-toc-modified-id=\"Remark-Tolerance-level-and-conergence-criteria-for-HJB-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Remark Tolerance level and conergence criteria for HJB</a></span></li></ul></li><li><span><a href=\"#Simulation\" data-toc-modified-id=\"Simulation-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Simulation</a></span></li><li><span><a href=\"#SCC-Calculation-Feyman-Kac\" data-toc-modified-id=\"SCC-Calculation-Feyman-Kac-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>SCC Calculation Feyman Kac</a></span><ul class=\"toc-item\"><li><span><a href=\"#Remark-Convergence-criteria-for-Feyman-Kac\" data-toc-modified-id=\"Remark-Convergence-criteria-for-Feyman-Kac-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>Remark Convergence criteria for Feyman Kac</a></span></li></ul></li><li><span><a href=\"#Probabilities\" data-toc-modified-id=\"Probabilities-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Probabilities</a></span></li></ul></li><li><span><a href=\"#Results\" data-toc-modified-id=\"Results-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Results</a></span><ul class=\"toc-item\"><li><span><a href=\"#Worst-case-probabilities-for-9-growth-damage-models\" data-toc-modified-id=\"Worst-case-probabilities-for-9-growth-damage-models-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Worst case probabilities for 9 growth damage models</a></span></li><li><span><a href=\"#Social-Cost-of-Carbon-Decomposition\" data-toc-modified-id=\"Social-Cost-of-Carbon-Decomposition-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Social Cost of Carbon Decomposition</a></span></li><li><span><a href=\"#Emission-trajectory\" data-toc-modified-id=\"Emission-trajectory-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Emission trajectory</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code and Illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T20:19:05.173106Z",
     "start_time": "2019-12-18T20:19:03.861634Z"
    }
   },
   "outputs": [],
   "source": [
    "# Required packages\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('./src')\n",
    "from supportfunctions import *\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing key parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T20:19:05.179703Z",
     "start_time": "2019-12-18T20:19:05.174681Z"
    }
   },
   "outputs": [],
   "source": [
    "Œæp =  1  / 175  # Ambiguity Aversion Paramter \n",
    "# We stored solutions for Œæp =  1 / 175 to which we referred as \"Ambiguity Averse\" and Œæp = 1000 as ‚ÄúAmbiguity Neutral‚Äù in the paper\n",
    "# Sensible choices are from 0.0002 to 4000, while for parameters input over 0.01 the final results won't alter much\n",
    "\n",
    "if Œæp < 1:\n",
    "    aversespec = \"Averse\"\n",
    "else:\n",
    "    aversespec = 'Neutral'\n",
    "\n",
    "v0_guess = None\n",
    "base_guess = None\n",
    "worst_guess = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T20:19:05.220564Z",
     "start_time": "2019-12-18T20:19:05.181664Z"
    }
   },
   "outputs": [],
   "source": [
    "# Don't run this cell if you want to solve without smart guess\n",
    "# Loading Smart guesses\n",
    "guess = pickle.load(open('./data/Growth{}guess.pickle'.format(aversespec), \"rb\", -1))\n",
    "v0_guess = guess['v0']\n",
    "q_guess = guess['q']\n",
    "e_guess = guess['e']\n",
    "base_guess = guess['base']\n",
    "worst_guess = guess['worst']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving HJB for Growth Damage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to solve the HJB corresponding to the planner's problem for the version of the model with climate damages to growth, we use the numerical method as we did for the model version with climate damages to consumption. The main difference comes from the expressions for the coefficients $A(x), B(x), C(x)$, and $D(x)$, which are now given by\n",
    "\n",
    "\\begin{eqnarray}\n",
    "A(x) &=& - \\delta \\\\\n",
    "B(x) &=& {\\widehat \\mu}(x)\\\\\n",
    "C(x) &=& {\\frac 1 2} \\sigma_X(x)'I\\sigma_X(x) \\\\\n",
    "D(x) &=& \\delta (1 - \\kappa) \\left( \\log \\left[ {\\alpha}  - {\\widehat i}(x)  - {\\widehat j} (x)  \\right] + k  \\right) + \\delta \\kappa \\left[ \\log {\\widehat e}(x)   +  r \\right] + {\\frac {\\xi_m} 2} {\\widehat g}(x) \\cdot {\\widehat g}(x)   + \\xi_p {\\widehat {\\mathbb I}}(x) \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "where $D(x)$ no longer contains the climate damage impact, but instead ${\\widehat \\mu}(x)$ incorporates climate damages through its impact on the drift of capital."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark:  Accounting for uncertainty about growth damages\n",
    "\n",
    "\n",
    "In calculating growth damamage uncertainty, the term of interest is the contribution of smooth ambiguity to our planner's HJB equation:\n",
    "\n",
    "\\begin{equation}\n",
    "-\\Gamma_i(\\beta F)v_{k},\n",
    "\\end{equation}\n",
    "\n",
    "where the function $\\Gamma_i(\\beta f)$ is given by\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\Gamma_i(\\beta F)\t=\\gamma_{0,i}+\\gamma_{1,i}\\beta (F+\\tilde{F}/\\beta)+\\frac{\\gamma_{2,i}}{2}(\\beta (F+\\tilde{F}/\\beta))^{2}.\n",
    "\\end{eqnarray*}\n",
    "\n",
    "$\\tilde{F}$ is the pre-industrial global mean temperature before climate damages negatively affect growth based on empirical measurements and estimates from \\cite{BurkeDavisDiffenbaugh:2018}.\n",
    "\n",
    "\n",
    "##### Selecting the values of $\\gamma_1, \\gamma_2$\n",
    "\n",
    "In this setting, we take a draw of n values of $\\gamma = (\\gamma_1, \\gamma_2)$ based on the distributional structure provided \\cite{BurkeDavisDiffenbaugh:2018}:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\gamma \\sim \\mathcal{N}(\\mu, \\Sigma), \\\\\n",
    "\\mu = [\\mu_1, \\mu_2], \\\\\n",
    "\\Sigma = \\left[ \\begin{matrix}\n",
    "\\sigma_1^2 & \\rho \\\\\n",
    "\\rho & \\sigma_2^2\n",
    "\\end{matrix} \\right].\n",
    "\\end{eqnarray*}\n",
    "\n",
    "We select points based on the Hermite-Gaussian quadrature abscissas and give probability weights to these outcomes based on the pdf for $\\gamma$. \n",
    "\n",
    "\n",
    "##### Varying damage models\n",
    "\n",
    "We must proceed differently for the high damage model with numerical computation.\n",
    "Form\n",
    "\\begin{equation}\n",
    "{\\rm num}_i(\\beta)  = \\exp\\left( - {\\frac 1 \\xi_p} \\left[ -\\Gamma_i(\\beta F)v_{k}  \\right] \\right),\n",
    "\\end{equation}\n",
    "\n",
    "compute:\n",
    "\\begin{equation}\n",
    "{\\rm scale}_i = \\int_\\beta {\\rm num}_i(\\beta) p(\\beta) d\\beta\n",
    "\\end{equation}\n",
    "\n",
    "via Gauss-Hermite quadrature, \n",
    "and form a density conditioned on model two:\n",
    "\\begin{equation}\n",
    "{\\widetilde q}_i(\\beta)  = {\\frac {{\\rm num}_i(\\beta)}{{\\rm scale}_i}}\n",
    "\\end{equation}\n",
    "\n",
    "relative to $p(\\beta)$.  \n",
    "\n",
    "Also compute:\n",
    "\\begin{equation}\n",
    "{\\mathcal  I}_i =  -\\xi_p \\log {\\rm scale}_i,\n",
    "\\end{equation}\n",
    "\n",
    "along with:\n",
    "\\begin{equation}\n",
    "{\\mathcal J}_i = {\\frac {-\\Gamma_i(\\beta F)v_{k}}{{\\rm scale}_i}}\n",
    "\\end{equation}\n",
    "\n",
    "using Gaussian-Hermite quadrature for the numerator integral. \n",
    "Notice that ${\\mathcal J}_i$ can be computed for an arbitrary $e$ since the numerator integral does not depend on $e$.\n",
    " With these computations in hand, form relative entropy conditioned on model two:\n",
    "\\begin{equation}\n",
    "{\\mathcal R}_i = {\\frac 1 {\\xi_p}} \\left[{\\mathcal I}_i - {\\mathcal J}_i \\right] .\n",
    "\\end{equation}\n",
    "\n",
    "Notice that ${\\mathcal J}_i(e)$ can be computed for an arbitrary $e$ since the numerator integral does not depend on $e$.\n",
    "\n",
    "\n",
    "##### Distorted probabilities\n",
    "\n",
    "Worst-case probabilities over models satisfy:\n",
    "\\begin{equation}\n",
    "{\\widetilde \\pi}_i \\  \\ \\propto  \\  \\ {\\widehat \\pi}_i \\exp\\left( -{\\frac 1 {\\xi_p}} {\\mathcal I}_i \\right)\n",
    "\\end{equation}\n",
    "\n",
    "where the constant of proportionality scales the right-hand side so that the resulting ${\\tilde \\pi}_i$ sum to one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T20:19:07.775091Z",
     "start_time": "2019-12-18T20:19:05.222561Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "McD = np.loadtxt('./data/TCRE_MacDougallEtAl2017_update.txt')\n",
    "par_lambda_McD = McD / 1000\n",
    "\n",
    "Œ≤ùòß = np.mean(par_lambda_McD)  # Climate sensitivity parameter, MacDougall (2017)\n",
    "œÉ·µ¶ = np.var(par_lambda_McD, ddof=1)  # Variance of climate sensitivity parameters\n",
    "Œª = 1.0 / œÉ·µ¶ \n",
    "\n",
    "# Parameters are defined the same as they are in the paper\n",
    "Œ¥ = 0.01        \n",
    "Œ∫ = 0.032       \n",
    "œÉùò® = 0.02\n",
    "œÉùò¨ = 0.0161\n",
    "œÉùò≥ = 0.0339 \n",
    "Œ± = 0.115000000000000\n",
    "œï0 = 0.0600\n",
    "œï1 = 16.666666666666668\n",
    "Œºk = -0.034977443912449\n",
    "œà0 = 0.112733407891680\n",
    "œà1 = 0.142857142857143\n",
    "FÃÑ = 13\n",
    "\n",
    "# False trasient time step\n",
    "Œµ = 0.3\n",
    "# Specifying tolerance level\n",
    "tol = 1e-12\n",
    "# Cobweb learning rate\n",
    "Œ∑ = 0.05\n",
    "\n",
    "# Solving HJB\n",
    "R_min = 0\n",
    "R_max = 9\n",
    "F_min = 0\n",
    "F_max = 750\n",
    "K_min = 0\n",
    "K_max = 18\n",
    "\n",
    "\n",
    "# nR = 30\n",
    "# nF = 30\n",
    "# nK = 25\n",
    "\n",
    "# R = np.linspace(R_min, R_max, nR)\n",
    "# F = np.linspace(F_min, F_max, nF)\n",
    "# K = np.linspace(K_min, K_max, nK)\n",
    "\n",
    "# hR = R[1] - R[0]\n",
    "# hF = F[1] - F[0]\n",
    "# hK = K[1] - K[0]\n",
    "\n",
    "hR = 0.05\n",
    "hF = 25\n",
    "hK = 0.15\n",
    "\n",
    "R = np.arange(R_min, R_max + hR, hR)\n",
    "nR = len(R)\n",
    "F = np.arange(F_min, F_max + hF, hF)\n",
    "nF = len(F)\n",
    "K = np.arange(K_min, K_max + hK, hK)\n",
    "nK = len(K)\n",
    "\n",
    "\n",
    "# Here's how we discretize our state space formulating PDE, see Remark 2 in consumption setting\n",
    "(R_mat, F_mat, K_mat) = np.meshgrid(R,F,K, indexing = 'ij')\n",
    "stateSpace = np.hstack([R_mat.reshape(-1,1,order = 'F'),F_mat.reshape(-1,1,order = 'F'),K_mat.reshape(-1,1,order = 'F')])\n",
    "\n",
    "\n",
    "Œº1 = 1.272e-02\n",
    "Œº2 = -4.871e-04\n",
    "œÉ1 = 3.248e-03\n",
    "œÉ2 = 1.029e-04\n",
    "œÅ12 = -2.859133e-07\n",
    "\n",
    "œÉ2 = œÉ2 * 2\n",
    "œÅ12 = œÅ12 * 2\n",
    "\n",
    "Œº = np.matrix([-Œº1, -Œº2 * 2])\n",
    "œÉ = np.matrix([[œÉ1 ** 2, œÅ12], [œÅ12, œÉ2 ** 2]])\n",
    "Œ£ = np.matrix([[œÉ·µ¶,0,0],[0,œÉ1 ** 2, œÅ12],[0, œÅ12, œÉ2 ** 2]])\n",
    "\n",
    "[gam1,w1] = quad_points_hermite(3)\n",
    "gamm1 = np.sqrt(2) * 1 * gam1 + 0\n",
    "[gam2,w2] = quad_points_hermite(3)\n",
    "gamm2 = np.sqrt(2) * 1 * gam2 + 0\n",
    "\n",
    "At = np.linalg.cholesky(œÉ)\n",
    "x = np.zeros([2,9])\n",
    "tmp = [-Œº1,-Œº2 * 2]\n",
    "x[:,0] = tmp + At.dot([gamm1[0], gamm2[0]])\n",
    "x[:,1] = tmp + At.dot([gamm1[0], gamm2[1]])\n",
    "x[:,2] = tmp + At.dot([gamm1[0], gamm2[2]])\n",
    "x[:,3] = tmp + At.dot([gamm1[1], gamm2[0]])\n",
    "x[:,4] = tmp + At.dot([gamm1[1], gamm2[1]])\n",
    "x[:,5] = tmp + At.dot([gamm1[1], gamm2[2]])\n",
    "x[:,6] = tmp + At.dot([gamm1[2], gamm2[0]])\n",
    "x[:,7] = tmp + At.dot([gamm1[2], gamm2[1]])\n",
    "x[:,8] = tmp + At.dot([gamm1[2], gamm2[2]])\n",
    "\n",
    "w = np.array([[w1[0],w1[0],w1[0],w1[1],w1[1],w1[1],w1[2],w1[2],w1[2]],\n",
    "               [w2[0],w2[1],w2[2],w2[0],w2[1],w2[2],w2[0],w2[1],w2[2]]])\n",
    "gamma1 = x[0,:]\n",
    "gamma2 = x[1,:]\n",
    "wgt1 = w[0,:]\n",
    "wgt2 = w[1,:]\n",
    "\n",
    "vals = np.linspace(0,30,100)\n",
    "\n",
    "weight = np.zeros(9)\n",
    "total_weight = 0\n",
    "for ite in range(9):\n",
    "    weight[ite] = wgt1[ite] * wgt2[ite]\n",
    "    total_weight += weight[ite]\n",
    "    \n",
    "weight = weight / total_weight\n",
    "\n",
    "gamma0 = np.zeros(9)\n",
    "for ite in range(9):\n",
    "    gamma0[ite] = max(-(gamma1[ite] * vals + 0.5 * gamma2[ite] * vals ** 2))\n",
    "    \n",
    "v0 = Œ∫ * R_mat + (1-Œ∫) * K_mat\n",
    "\n",
    "FC_Err = 1\n",
    "episode = 0\n",
    "\n",
    "if v0_guess is not None:\n",
    "    v0 = v0_guess\n",
    "    q = q_guess\n",
    "    e_star = e_guess\n",
    "    episode = 1\n",
    "    Œµ = 0.1\n",
    "\n",
    "\n",
    "PDE_errs = []\n",
    "FC_errs = []\n",
    "\n",
    "while episode == 0 or FC_Err > tol:\n",
    "    vold = v0.copy()\n",
    "    # Applying finite difference scheme to the value function\n",
    "    v0_dr = finiteDiff(v0,0,1,hR) \n",
    "    v0_df = finiteDiff(v0,1,1,hF)\n",
    "    v0_dk = finiteDiff(v0,2,1,hK)\n",
    "\n",
    "    v0_drr = finiteDiff(v0,0,2,hR)\n",
    "    v0_drr[v0_dr < 1e-16] = 0\n",
    "    v0_dr[v0_dr < 1e-16] = 1e-16\n",
    "    v0_dff = finiteDiff(v0,1,2,hF)\n",
    "    v0_dkk = finiteDiff(v0,2,2,hK)\n",
    "    if episode > 2000:\n",
    "        Œµ = 0.1\n",
    "    elif episode > 1000:\n",
    "        Œµ = 0.2\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    if episode == 0:\n",
    "        Acoeff = np.ones(R_mat.shape)\n",
    "        Bcoeff = ((Œ¥ * (1 - Œ∫) * œï1 + œï0 * œï1 * v0_dk) * Œ¥ * (1 - Œ∫) / (v0_dr * œà0 * 0.5) * np.exp(0.5 * (R_mat - K_mat))) / (Œ¥ * (1 - Œ∫) * œï1)\n",
    "        Ccoeff = -Œ±  - 1 / œï1\n",
    "        j = ((-Bcoeff + np.sqrt(Bcoeff ** 2 - 4 * Acoeff * Ccoeff)) / (2 * Acoeff)) ** 2\n",
    "        i = Œ± - j - (Œ¥ * (1 - Œ∫)) / (v0_dr * œà0 * 0.5) * j ** 0.5 * np.exp(0.5 * (R_mat - K_mat))\n",
    "        q = Œ¥ * (1 - Œ∫) / (Œ± - i - j)        \n",
    "    else:\n",
    "        e_hat = e_star\n",
    "        \n",
    "        # Cobeweb scheme to update controls i and j; q is an intermediary variable that determines i and j\n",
    "        Converged = 0\n",
    "        nums = 0\n",
    "        while Converged == 0:\n",
    "            i_star = (œï0 * œï1 * v0_dk / q - 1) / œï1\n",
    "            j_star = (q * np.exp(œà1 * (R_mat - K_mat)) / (v0_dr * œà0 * œà1)) ** (1 / (œà1 - 1))\n",
    "            if Œ± > np.max(i_star + j_star):\n",
    "                q_star = Œ∑ * Œ¥ * (1 - Œ∫) / (Œ± - i_star - j_star) + (1 - Œ∑) * q\n",
    "            else:\n",
    "                q_star = 2 * q\n",
    "            if np.max(abs(q - q_star) / Œ∑) <= 1e-5:\n",
    "                Converged = 1\n",
    "                q = q_star\n",
    "                i = i_star\n",
    "                j = j_star\n",
    "            else:\n",
    "                q = q_star\n",
    "                i = i_star\n",
    "                j = j_star\n",
    "            \n",
    "            nums += 1\n",
    "            \n",
    "#         if episode % 100 == 0:\n",
    "        print('Cobweb Passed, iterations: {}, i error: {:10f}, j error: {:10f}'.format(nums, np.max(i - i_star), np.max(j - j_star)))\n",
    "        \n",
    "    # calculating growth damage uncertainties, see remark 2.1.1 for details\n",
    "    a_ = [] \n",
    "    b_ = [] \n",
    "    c_ = [] \n",
    "    ŒªÃÉ_ = [] \n",
    "    Œ≤ÃÉ_ = [] \n",
    "    I_ = [] \n",
    "    R_ = [] \n",
    "    œÄÃÉ_ = [] \n",
    "    J_ = []\n",
    "\n",
    "    for ite in range(9):\n",
    "        a_.append( -v0_dk * (gamma0[ite] + gamma1[ite] * FÃÑ + 0.5 * gamma2[ite] * FÃÑ ** 2) )\n",
    "        b_.append( -v0_dk * F_mat * (gamma1[ite] + gamma2[ite] * FÃÑ) )\n",
    "        c_.append( -v0_dk * gamma2[ite] * F_mat ** 2 )\n",
    "        ŒªÃÉ_.append( Œª + c_[ite] / Œæp )\n",
    "        Œ≤ÃÉ_.append( Œ≤ùòß - c_[ite] / Œæp / ŒªÃÉ_[ite] * Œ≤ùòß - b_[ite] / (Œæp * ŒªÃÉ_[ite]))\n",
    "        I_.append( a_[ite] - 0.5 * np.log(Œª) * Œæp + 0.5 * np.log(ŒªÃÉ_[ite]) * Œæp + 0.5 * Œª * Œ≤ùòß ** 2 * Œæp - 0.5 * ŒªÃÉ_[ite] * (Œ≤ÃÉ_[ite]) ** 2 * Œæp )\n",
    "        œÄÃÉ_.append( weight[ite] * np.exp(-1 / Œæp * I_[ite]) )\n",
    "        J_.append( a_[ite] + b_[ite] * Œ≤ÃÉ_[ite] + 0.5 * c_[ite] * Œ≤ÃÉ_[ite] ** 2 + 0.5 * c_[ite] / ŒªÃÉ_[ite] )\n",
    "        R_.append((I_[ite] - J_[ite]) / Œæp)\n",
    "\n",
    "\n",
    "    œÄÃÉ_total = sum(œÄÃÉ_)\n",
    "    œÄÃÉ_norm_ = œÄÃÉ_ / œÄÃÉ_total\n",
    "\n",
    "    B1 = v0_dr - v0_df * np.exp(R_mat)\n",
    "    C1 = Œ¥ * Œ∫\n",
    "    e = C1 / B1\n",
    "    e_star = e\n",
    "\n",
    "    I_term = -1 * Œæp * np.log(sum(œÄÃÉ_))\n",
    "    drift_distort = sum([x*y for (x,y) in zip(œÄÃÉ_norm_, J_)])\n",
    "    RE = sum(x * y + x * np.log(x / z) for (x,y,z) in zip(œÄÃÉ_norm_, R_, weight))\n",
    "    RE_total = Œæp * RE\n",
    "\n",
    "    # Formulating coefficients for the HJB False Transient problem, See remark 4 for more details\n",
    "    A = -Œ¥ * np.ones(R_mat.shape)\n",
    "    B_r = -e_star + œà0 * (j ** œà1) * np.exp(œà1 * (K_mat - R_mat)) - 0.5 * (œÉùò≥ ** 2)\n",
    "    B_k = Œºk + œï0 * np.log(1 + i * œï1) - 0.5 * (œÉùò¨ ** 2)\n",
    "    B_f = e_star * np.exp(R_mat)\n",
    "    C_rr = 0.5 * œÉùò≥ ** 2 * np.ones(R_mat.shape)\n",
    "    C_kk = 0.5 * œÉùò¨ ** 2 * np.ones(R_mat.shape)\n",
    "    C_ff = np.zeros(R_mat.shape)\n",
    "\n",
    "    D = Œ¥ * Œ∫ * np.log(e_star) + Œ¥ * Œ∫ * R_mat + Œ¥ * (1 - Œ∫) * (np.log(Œ± - i - j) + K_mat) + I_term #  + drift_distort + RE_total\n",
    "    \n",
    "    # Solve linear system using a conjugate-gradient method in C++, see remark 5, 6 for more details\n",
    "    out = PDESolver(stateSpace, A, B_r, B_f, B_k, C_rr, C_ff, C_kk, D, v0, Œµ, solverType='False Transient')\n",
    "\n",
    "    out_comp = out[2].reshape(v0.shape,order = \"F\")\n",
    "    \n",
    "    # Calculating PDE Error and False Transient (lhs) Error\n",
    "    PDE_rhs = A * v0 + B_r * v0_dr + B_f * v0_df + B_k * v0_dk + C_rr * v0_drr + C_kk * v0_dkk + C_ff * v0_dff + D\n",
    "    PDE_Err = np.max(abs(PDE_rhs))\n",
    "    PDE_errs.append(PDE_Err)\n",
    "    FC_Err = np.max(abs((out_comp - v0)))\n",
    "    FC_errs.append(FC_Err)\n",
    "    if episode % 100 == 0:\n",
    "        print(\"Episode {:d}: PDE Error: {:.10f}; False Transient Error: {:.10f}; Iterations: {:d}; CG Error: {:.10f}\" .format(episode, PDE_Err, FC_Err, out[0], out[1]))\n",
    "    episode += 1\n",
    "    v0 = out_comp\n",
    "\n",
    "print(\"Episode {:d}: PDE Error: {:.10f}; False Transient Error: {:.10f}; Iterations: {:d}; CG Error: {:.10f}\" .format(episode, PDE_Err, FC_Err, out[0], out[1]))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark Tolerance level and conergence criteria for HJB\n",
    "\n",
    "Just as we did for the damages to consumption model, we use variable step sizes $\\epsilon$ to reduce the time needed for solving the model while ensuring convergence. We use $\\epsilon= 0.3$ for the first 1000 iterations, $\\epsilon= 0.2$ for iterations 1001-2000, and $\\epsilon= 0.1$ thereafter.\n",
    "\n",
    "The table below lists the convergence criteria and final errors for solving the HJB equations for the different growth damages settings. For solving the model with damages to growth, \n",
    "\n",
    "\n",
    "The table below lists the convergence criteria and final errors for solving the HJB equations for the different growth damages settings. For solving the model with damages to growth, we apply a two-stage convergence approach: we first solve the problem at a looser convergence tolerance (CG tolerance at 1e-10 and outer loop tolerance at 1e-8); then we import this solution and solve again using a tighter convergence criteria as listed in the table\n",
    "\n",
    "| ambiguity    |    stage    | Cobweb tolerance |  outer loop tolerance  | CG tolerance |  PDE Error | dt | iterations | time|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|averse | 1  | 1e-5 | 1e-08 | 1e-10 | 1.55e-5 |variable dt| 10587 | 2.5 hrs\n",
    "|averse | 2  | 1e-5 | 1e-12 | 1e-14 | 1.55e-5 | 0.1 | 20819(including stage 1) | 2 hrs\n",
    "|neutral  | 1  | 1e-5 | 1e-08 | 1e-10 | 1.27e-5 | 0.1 | 12855 | 3 hrs\n",
    "|neutral  | 2  | 1e-5 | 1e-12 | 1e-14 | 1.28e-5 | 0.1 |24857(including stage 1) | 2.5 hrs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T20:19:07.784067Z",
     "start_time": "2019-12-18T20:19:07.776089Z"
    }
   },
   "outputs": [],
   "source": [
    "class GridInterp():\n",
    "\n",
    "    def __init__(self, grids, values, method = 'Linear'):\n",
    "\n",
    "        # unpacking\n",
    "        self.grids = grids\n",
    "        (self.xs, self.ys, self.zs) = grids\n",
    "        self.nx = len(self.xs)\n",
    "        self.ny = len(self.ys)\n",
    "        self.nz = len(self.zs)\n",
    "        \n",
    "        self.values = values\n",
    "\n",
    "        assert (self.nx, self.ny, self.nz) == values.shape, \"ValueError: Dimensions not match\"\n",
    "        self.method = method\n",
    "\n",
    "    def get_value(self, x, y, z):\n",
    "\n",
    "        if self.method == 'Linear':\n",
    "            \n",
    "            func = RegularGridInterpolator(self.grids, self.values)\n",
    "            return func([x,y,z])[0]\n",
    "\n",
    "        elif self.method == 'Spline':\n",
    "\n",
    "            func1 = CubicSpline(self.xs, self.values)\n",
    "            yzSpace = func1(x)\n",
    "            \n",
    "            func2 = CubicSpline(self.ys, yzSpace)\n",
    "            zSpace = func2(y)\n",
    "            \n",
    "            func3 = CubicSpline(self.zs, zSpace)\n",
    "            return func3(z)\n",
    "\n",
    "        else:\n",
    "            raise ValueError('Method Not Supported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T20:19:13.457104Z",
     "start_time": "2019-12-18T20:19:07.785064Z"
    }
   },
   "outputs": [],
   "source": [
    "method = 'Linear'\n",
    "T = 100\n",
    "pers = 4 * T\n",
    "dt = T / pers\n",
    "nDims = 4\n",
    "its = 1\n",
    "\n",
    "gridpoints = (R, F, K)\n",
    "\n",
    "# emissions\n",
    "e_func_r = GridInterp(gridpoints, e, method)\n",
    "def e_func(x):\n",
    "    return e_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "# investment in reserves\n",
    "j_func_r = GridInterp(gridpoints, j, method)\n",
    "def j_func(x):\n",
    "    return max(j_func_r.get_value(np.log(x[0]), x[2], np.log(x[1])), 0)\n",
    "\n",
    "# investment in productive capital\n",
    "i_func_r = GridInterp(gridpoints, i, method)\n",
    "def i_func(x):\n",
    "    return i_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "# first derivative of Value function w.r.t R\n",
    "v_drfunc_r = GridInterp(gridpoints, v0_dr, method)\n",
    "def v_drfunc(x):\n",
    "    return v_drfunc_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "# first derivative of Value function w.r.t F\n",
    "v_dffunc_r = GridInterp(gridpoints, v0_df, method)\n",
    "def v_dffunc(x):\n",
    "    return v_dffunc_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "# first derivative of Value function w.r.t k\n",
    "v_dkfunc_r = GridInterp(gridpoints, v0_dk, method)\n",
    "def v_dkfunc(x):\n",
    "    return v_dkfunc_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "# Value function\n",
    "v_func_r = GridInterp(gridpoints, v0, method)\n",
    "def v_func(x):\n",
    "    return v_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "# Distorted model probabilities\n",
    "œÄÃÉ_norm_func = []\n",
    "œÄÃÉ_norm_func_r = []\n",
    "\n",
    "for ite in range(len(œÄÃÉ_norm_)):\n",
    "    œÄÃÉ_norm_func_r.append(GridInterp(gridpoints, œÄÃÉ_norm_[ite], method))\n",
    "    œÄÃÉ_norm_func.append(lambda x, ite = ite: œÄÃÉ_norm_func_r[ite].get_value(np.log(x[0]), x[2], np.log(x[1])))\n",
    "\n",
    "# Relaive entropies\n",
    "RE_func_r = GridInterp(gridpoints, RE, method)\n",
    "def RE_func(x):\n",
    "    return RE_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "a = []\n",
    "b = []\n",
    "c = []\n",
    "dmg_tilt_ = []\n",
    "dmg_ = []\n",
    "base_driftK_r = []\n",
    "base_driftK = []\n",
    "tilt_driftK_r = []\n",
    "tilt_driftK = []\n",
    "\n",
    "for ite in range(len(œÄÃÉ_norm_)):\n",
    "    a.append(gamma0[ite] + gamma1[ite] * FÃÑ + 0.5 * gamma2[ite] * FÃÑ ** 2)\n",
    "    b.append(F_mat * (gamma1[ite] + gamma2[ite] * FÃÑ))\n",
    "    c.append(gamma2[ite] * F_mat ** 2)\n",
    "    dmg_tilt_.append(a[ite] + b[ite] * Œ≤ÃÉ_[ite] + 0.5 * c[ite] * \n",
    "                     (Œ≤ÃÉ_[ite] ** 2) + 0.5 * c[ite] / ŒªÃÉ_[ite])\n",
    "    dmg_.append(a[ite] + b[ite] * Œ≤f + 0.5 * c[ite] * Œ≤f ** 2 + 0.5 * c[ite] / Œª)\n",
    "    # drift distortions\n",
    "    base_driftK_r.append(GridInterp(gridpoints, dmg_[ite], method))\n",
    "    base_driftK.append(lambda x, ite = ite: base_driftK_r[ite].get_value(np.log(x[0]), x[2], np.log(x[1])))\n",
    "    tilt_driftK_r.append(GridInterp(gridpoints, dmg_tilt_[ite], method))\n",
    "    tilt_driftK.append(lambda x, ite = ite: tilt_driftK_r[ite].get_value(np.log(x[0]), x[2], np.log(x[1])))\n",
    "    \n",
    "# Weighted sum of drift distortion\n",
    "def Gamma_base(x):\n",
    "    res = 0\n",
    "    for ite in range(len(weight)):\n",
    "        res += weight[ite] * base_driftK[ite](x)\n",
    "    return res\n",
    "\n",
    "def Gamma_tilted(x):\n",
    "    res = 0\n",
    "    for ite in range(len(weight)):\n",
    "        res += œÄÃÉ_norm_func[ite](x) * tilt_driftK[ite](x)\n",
    "    return res\n",
    "\n",
    "# drifts for each diffusion process\n",
    "def muR(x):\n",
    "    return -e_func(x) + œà0 * (j_func(x) * x[1] / x[0]) ** œà1\n",
    "def muK_tilted(x): \n",
    "    return (Œºk + œï0 * np.log(1 + i_func(x) * œï1)- Gamma_tilted(x))\n",
    "def muK_base(x): \n",
    "    return (Œºk + œï0 * np.log(1 + i_func(x) * œï1)- Gamma_base(x))\n",
    "def muF(x):\n",
    "    return e_func(x) * x[0]\n",
    "\n",
    "# volatilities for each diffusion process\n",
    "def sigmaR(x):\n",
    "    return np.zeros(x[:4].shape)\n",
    "def sigmaK(x):\n",
    "    return np.zeros(x[:4].shape)\n",
    "def sigmaF(x):\n",
    "    return np.zeros(x[:4].shape)\n",
    "\n",
    "# initial points\n",
    "R_0 = 650\n",
    "K_0 = 80 / Œ±\n",
    "F_0 = 870 - 580\n",
    "initial_val = np.array([R_0, K_0, F_0])\n",
    "\n",
    "# Set bounds\n",
    "R_max_sim = np.exp(max(R))\n",
    "K_max_sim = np.exp(max(K))\n",
    "F_max_sim = max(F)\n",
    "\n",
    "R_min_sim = np.exp(min(R))\n",
    "K_min_sim = np.exp(min(K))\n",
    "F_min_sim = min(F)\n",
    "\n",
    "upperbounds = np.array([R_max_sim, K_max_sim, F_max_sim, K_max_sim])\n",
    "lowerbounds = np.array([R_min_sim, K_min_sim, F_min_sim, K_min_sim])\n",
    "\n",
    "hists = np.zeros([pers, nDims, its])\n",
    "e_hists = np.zeros([pers,its])\n",
    "j_hists = np.zeros([pers,its])\n",
    "i_hists = np.zeros([pers,its])\n",
    "RE_hists = np.zeros([pers,its])\n",
    "\n",
    "pi_tilde_hists = []\n",
    "for ite in range(len(œÄÃÉ_norm_)):\n",
    "    pi_tilde_hists.append(np.zeros([pers,its]))\n",
    "\n",
    "for iters in range(its):\n",
    "    hist = np.zeros([pers,nDims])\n",
    "    e_hist = np.zeros([pers,1])\n",
    "    i_hist = np.zeros([pers,1])\n",
    "    j_hist = np.zeros([pers,1])\n",
    "    \n",
    "    RE_hist = np.zeros([pers,1])\n",
    "    pi_tilde_hist = []\n",
    "    for ite in range(len(œÄÃÉ_norm_)):\n",
    "        pi_tilde_hist.append(np.zeros([pers,1]))\n",
    "    \n",
    "    hist[0,:] = [R_0, K_0, F_0, K_0]\n",
    "    e_hist[0] = e_func(hist[0,:]) * hist[0,0]\n",
    "    i_hist[0] = i_func(hist[0,:]) * hist[0,1]\n",
    "    j_hist[0] = j_func(hist[0,:]) * hist[0,0]\n",
    "    RE_hist[0] = RE_func(hist[0,:])\n",
    "    \n",
    "    for ite in range(len(œÄÃÉ_norm_)):\n",
    "        pi_tilde_hist[ite][0] = œÄÃÉ_norm_func[ite](hist[0,:])\n",
    "    \n",
    "    for tm in range(1,pers):\n",
    "        shock = norm.rvs(0,np.sqrt(dt),nDims)\n",
    "        hist[tm,0] = cap(hist[tm-1,0] * np.exp((muR(hist[tm-1,:])- 0.5 * sum((sigmaR(hist[tm-1,:])) ** 2))* dt + sigmaR(hist[tm-1,:]).dot(shock)),lowerbounds[0], upperbounds[0])\n",
    "        hist[tm,1] = cap(hist[tm-1,1] * np.exp((muK_base(hist[tm-1,:])- 0.5 * sum((sigmaK(hist[tm-1,:])) ** 2))* dt + sigmaK(hist[tm-1,:]).dot(shock)),lowerbounds[1], upperbounds[1])\n",
    "        hist[tm,2] = cap(hist[tm-1,2] + muF(hist[tm-1,:]) * dt + sigmaF(hist[tm-1,:]).dot(shock), lowerbounds[2], upperbounds[2])\n",
    "        hist[tm,3] = cap(hist[tm-1,3] * np.exp((muK_tilted(hist[tm-1,:])- 0.5 * sum((sigmaK(hist[tm-1,:])) ** 2))* dt + sigmaK(hist[tm-1,:]).dot(shock)),lowerbounds[3], upperbounds[3])\n",
    "\n",
    "        e_hist[tm] = e_func(hist[tm-1,:]) * hist[tm-1,0]\n",
    "        i_hist[tm] = i_func(hist[tm-1,:]) * hist[tm-1,1]\n",
    "        j_hist[tm] = j_func(hist[tm-1,:]) * hist[tm-1,1]\n",
    "        RE_hist[tm] = RE_func(hist[tm-1, :])\n",
    "        \n",
    "        for ite in range(len(œÄÃÉ_norm_)):\n",
    "            pi_tilde_hist[ite][tm] = œÄÃÉ_norm_func[ite](hist[tm-1,:])\n",
    "\n",
    "    hists[:,:,iters] = hist\n",
    "    e_hists[:,[iters]] = e_hist\n",
    "    i_hists[:,[iters]] = i_hist\n",
    "    j_hists[:,[iters]] = j_hist\n",
    "    \n",
    "    RE_hists[:,[iters]] =  RE_hist\n",
    "    \n",
    "    for ite in range(len(œÄÃÉ_norm_)):\n",
    "        pi_tilde_hists[ite][:,[iters]] = pi_tilde_hist[ite]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCC Calculation Feyman Kac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The growth damages model setting is solved using the same numerical method as the consumption damage model, with a few minor adjustments made to the equations and calculations. First, the expressions for $\\Psi^*(x)$ and $\\bar{\\Psi}(x)$ are now given by \n",
    "\n",
    "\\begin{align*}\n",
    " \\Psi^*(x) & = -  \\int_{\\Theta} V_k(x) \\left[ \\nabla \\Gamma(\\beta f) \\beta \\right] q^*(\\theta | x) P(d\\theta) \\\\\n",
    " \\bar{\\Psi}(x) & = -  \\int_{\\Theta} V_k(x) \\left[ \\nabla \\Gamma(\\beta f) \\beta \\right] P(d\\theta).\n",
    "\\end{align*}\n",
    "\n",
    "Second, these integral expressions can be calculated using entirely analytical simplifictions as will be outlined below.\n",
    "\n",
    "Finally, there are no longer direct contributions to the damage evolution that have integral expressions that need to be numerically calculated and added to the PDEs derived from application of the F-K formula.\n",
    "\n",
    "With these differences we can apply the analogous approach to the consumption damage setting to derive $ecc^*$, $\\bar{ecc}$, and $ucc^*$ for the growth damages model setting. As before, after imposing the same finite difference scheme the PDEs are characterized by unconditionally linear equations that we can solve directly by approximately inverting the linear systems using the conjugate gradient method without the use of the false transient or other iterative schemes. These PDEs can also be alternatively be solved using the implicit, finite-difference scheme with the conjugate gradient solver augmented by a false transient where the coefficients are definied as in the consumption damage case, appropriately adjusted for the $\\mu^*_X[x,a^*(x)]$, $\\mu_X[x,a^*(x)]$, $\\Psi^*(x)$, and $\\bar{\\Psi}(x)$ that correspond to the growth damages model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T20:19:16.884843Z",
     "start_time": "2019-12-18T20:19:13.458675Z"
    }
   },
   "outputs": [],
   "source": [
    "a, b, c, dmg_ = ([] for ite in range(4)) \n",
    "for ite in range(len(œÄÃÉ_norm_)):\n",
    "    a.append(np.zeros(F_mat.shape) )\n",
    "    b.append(v0_dk * (gamma1[ite] + gamma2[ite] * FÃÑ))\n",
    "    c.append(2 * v0_dk * gamma2[ite] * F_mat)\n",
    "    dmg_.append(a[ite] + b[ite] * Œ≤f + 0.5 * c[ite] * Œ≤f ** 2 + 0.5 * c[ite] / Œª)\n",
    "    \n",
    "flow_base = sum(w * d for w, d in zip(weight, dmg_))\n",
    "\n",
    "a, b, c, dmg_ = ([] for ite in range(4)) \n",
    "for ite in range(len(œÄÃÉ_norm_)):\n",
    "    a.append(v0_dk * (gamma0[ite] + gamma1[ite] * FÃÑ + 0.5 * gamma2[ite] * FÃÑ ** 2))\n",
    "    b.append(v0_dk * F_mat * (gamma1[ite] + gamma2[ite] * FÃÑ))\n",
    "    c.append(v0_dk * gamma2[ite] * F_mat ** 2)\n",
    "    dmg_.append(a[ite] + b[ite] * Œ≤f + 0.5 * c[ite] * Œ≤f ** 2 + 0.5 * c[ite] / Œª)\n",
    "\n",
    "Gamma_base = sum(w * d for w, d in zip(weight, dmg_))\n",
    "\n",
    "A = -Œ¥ * np.ones(R_mat.shape)\n",
    "# B_r = -e + œà0 * (j ** œà1) - 0.5 * (œÉùò≥ ** 2)\n",
    "B_r = -e + œà0 * (j ** œà1) * np.exp(œà1 * (K_mat - R_mat)) - 0.5 * (œÉùò≥ ** 2)\n",
    "B_k = Œºk + œï0 * np.log(1 + i * œï1) - 0.5 * (œÉùò¨ ** 2) - Gamma_base\n",
    "B_f = e * np.exp(R_mat)\n",
    "C_rr = 0.5 * œÉùò≥ ** 2 * np.ones(R_mat.shape)\n",
    "C_kk = 0.5 * œÉùò¨ ** 2 * np.ones(R_mat.shape)\n",
    "C_ff = np.zeros(R_mat.shape)\n",
    "D = flow_base\n",
    "\n",
    "if base_guess is None:\n",
    "    base_guess = np.zeros(R_mat.shape)\n",
    "\n",
    "\n",
    "# out = PDESolver(stateSpace, A, B_r, B_f, B_k, C_rr, C_ff, C_kk, D, v0, Œµ, 'Feyman Kac')\n",
    "out = PDESolver(stateSpace, A, B_r, B_f, B_k, C_rr, C_ff, C_kk, D, base_guess, Œµ, solverType='Feyman Kac')\n",
    "v0_base = out[2].reshape(v0.shape, order=\"F\")\n",
    "v0_base = v0_base\n",
    "\n",
    "v0_dr_base = finiteDiff(v0_base,0,1,hR,1e-16) \n",
    "v0_df_base = finiteDiff(v0_base,1,1,hF)\n",
    "v0_dk_base = finiteDiff(v0_base,2,1,hK)\n",
    "\n",
    "v0_drr_base = finiteDiff(v0_base,0,2,hR)\n",
    "v0_dff_base = finiteDiff(v0_base,1,2,hF)\n",
    "v0_dkk_base = finiteDiff(v0_base,2,2,hK)\n",
    "\n",
    "\n",
    "PDE_rhs = A * v0_base + B_r * v0_dr_base + B_f * v0_df_base + B_k * v0_dk_base + C_rr * v0_drr_base + C_kk * v0_dkk_base + C_ff * v0_dff_base + D\n",
    "PDE_Err = np.max(abs(PDE_rhs))\n",
    "print(\"Feyman Kac Base Model Solved. PDE Error: {:.10f}; Iterations: {:d}; CG Error: {:.10f}\".format(PDE_Err, out[0], out[1]))\n",
    "\n",
    "a, b, c, dmg_tilt_ = ([] for ite in range(4)) \n",
    "for ite in range(len(œÄÃÉ_norm_)):\n",
    "    a.append(np.zeros(F_mat.shape) )\n",
    "    b.append(v0_dk * (gamma1[ite] + gamma2[ite] * FÃÑ))\n",
    "    c.append(2 * v0_dk * gamma2[ite] * F_mat)\n",
    "    dmg_tilt_.append(a[ite] + b[ite] * Œ≤ÃÉ_[ite] + 0.5 * c[ite] * Œ≤ÃÉ_[ite] ** 2 + 0.5 * c[ite] / ŒªÃÉ_[ite])\n",
    "    \n",
    "flow_tilted = sum(w * d for w, d in zip(œÄÃÉ_norm_, dmg_tilt_))\n",
    "\n",
    "a, b, c, dmg_tilt_ = ([] for ite in range(4)) \n",
    "for ite in range(len(œÄÃÉ_norm_)):\n",
    "    a.append(v0_dk * (gamma0[ite] + gamma1[ite] * FÃÑ + 0.5 * gamma2[ite] * FÃÑ ** 2))\n",
    "    b.append(v0_dk * F_mat * (gamma1[ite] + gamma2[ite] * FÃÑ))\n",
    "    c.append(v0_dk * gamma2[ite] * F_mat ** 2)\n",
    "    dmg_tilt_.append(a[ite] + b[ite] * Œ≤ÃÉ_[ite] + 0.5 * c[ite] * Œ≤ÃÉ_[ite] ** 2 + 0.5 * c[ite] / ŒªÃÉ_[ite])\n",
    "\n",
    "Gamma_tilted = sum(w * d for w, d in zip(œÄÃÉ_norm_, dmg_tilt_))\n",
    "\n",
    "A = -Œ¥ * np.ones(R_mat.shape)\n",
    "# B_r = -e + œà0 * (j ** œà1) - 0.5 * (œÉùò≥ ** 2)\n",
    "B_r = -e + œà0 * (j ** œà1) * np.exp(œà1 * (K_mat - R_mat)) - 0.5 * (œÉùò≥ ** 2)\n",
    "B_k = Œºk + œï0 * np.log(1 + i * œï1) - 0.5 * (œÉùò¨ ** 2) - Gamma_tilted\n",
    "B_f = e * np.exp(R_mat)\n",
    "C_rr = 0.5 * œÉùò≥ ** 2 * np.ones(R_mat.shape)\n",
    "C_kk = 0.5 * œÉùò¨ ** 2 * np.ones(R_mat.shape)\n",
    "C_ff = np.zeros(R_mat.shape)\n",
    "D = flow_tilted\n",
    "\n",
    "if worst_guess is None:\n",
    "    worst_guess = np.zeros(R_mat.shape)\n",
    "\n",
    "# out = PDESolver(stateSpace, A, B_r, B_f, B_k, C_rr, C_ff, C_kk, D, v0, Œµ, 'Feyman Kac')\n",
    "out = PDESolver(stateSpace, A, B_r, B_f, B_k, C_rr, C_ff, C_kk, D, worst_guess, Œµ, solverType='Feyman Kac')\n",
    "v0_worst = out[2].reshape(v0.shape, order=\"F\")\n",
    "v0_worst = v0_worst\n",
    "\n",
    "v0_dr_worst = finiteDiff(v0_worst,0,1,hR,1e-16) \n",
    "v0_df_worst = finiteDiff(v0_worst,1,1,hF)\n",
    "v0_dk_worst = finiteDiff(v0_worst,2,1,hK)\n",
    "\n",
    "v0_drr_worst = finiteDiff(v0_worst,0,2,hR)\n",
    "v0_dff_worst = finiteDiff(v0_worst,1,2,hF)\n",
    "v0_dkk_worst = finiteDiff(v0_worst,2,2,hK)\n",
    "\n",
    "PDE_rhs = A * v0_worst + B_r * v0_dr_worst + B_f * v0_df_worst + B_k * v0_dk_worst + C_rr * v0_drr_worst + C_kk * v0_dkk_worst + C_ff * v0_dff_worst + D\n",
    "PDE_Err = np.max(abs(PDE_rhs))\n",
    "print(\"Feyman Kac Worst Model Solved. PDE Error: {:.10f}; Iterations: {:d}; CG Error: {:.10f}\".format(PDE_Err, out[0], out[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T20:19:17.628278Z",
     "start_time": "2019-12-18T20:19:16.886838Z"
    }
   },
   "outputs": [],
   "source": [
    "# Total SCC\n",
    "MC = Œ¥ * (1-Œ∫) / (Œ± * np.exp(K_mat) - i * np.exp(K_mat) - j * np.exp(K_mat))\n",
    "ME = Œ¥ * Œ∫ / (e * np.exp(R_mat))\n",
    "SCC = 1000 * ME / MC\n",
    "SCC_func_r = GridInterp(gridpoints, SCC, method)\n",
    "\n",
    "def SCC_func(x): \n",
    "#     return SCC_func_r.get_value(np.log(x[0]), x[2], np.log(x[3]))\n",
    "    return SCC_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "# Private SCC\n",
    "ME1 = v0_dr * np.exp(-R_mat)\n",
    "SCC1 = 1000 * ME1 / MC\n",
    "SCC1_func_r = GridInterp(gridpoints, SCC1, method)\n",
    "def SCC1_func(x):\n",
    "#     return SCC1_func_r.get_value(np.log(x[0]), x[2], np.log(x[3]))\n",
    "    return SCC1_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "# Feyman Kac solution under original unadjusted probabilities\n",
    "ME2_base = v0_base\n",
    "SCC2_base = 1000 * ME2_base / MC\n",
    "SCC2_base_func_r = GridInterp(gridpoints, SCC2_base, method)\n",
    "def SCC2_base_func(x):\n",
    "#     return SCC2_base_func_r.get_value(np.log(x[0]), x[2], np.log(x[3]))\n",
    "    return SCC2_base_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "\n",
    "ME2_base_a = -v0_df\n",
    "SCC2_base_a = 1000 * ME2_base_a / MC\n",
    "SCC2_base_a_func_r = GridInterp(gridpoints, SCC2_base_a, method)\n",
    "def SCC2_base_a_func(x):\n",
    "#     return SCC2_base_a_func_r.get_value(np.log(x[0]), x[2], np.log(x[3]))\n",
    "    return SCC2_base_a_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "# Feyman Kac solution under ambiguity-adjusted probability\n",
    "ME2_tilt = v0_worst\n",
    "SCC2_tilt = 1000 * ME2_tilt / MC\n",
    "SCC2_tilt_func_r = GridInterp(gridpoints, SCC2_tilt, method)\n",
    "def SCC2_tilt_func(x):\n",
    "#     return SCC2_tilt_func_r.get_value(np.log(x[0]), x[2], np.log(x[3]))\n",
    "    return SCC2_tilt_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "# Interpolating via simulated trajectories \n",
    "SCC_values = np.zeros([pers,its])\n",
    "SCC1_values = np.zeros([pers,its])\n",
    "SCC2_base_values = np.zeros([pers,its])\n",
    "SCC2_tilt_values = np.zeros([pers,its])\n",
    "SCC2_base_a_values = np.zeros([pers,its])\n",
    "\n",
    "for tm in range(pers):   # time horizons\n",
    "    for path in range(its):    # Number of trajectories, currently it's 1\n",
    "        SCC_values[tm, path] = SCC_func(hists[tm,:,path])\n",
    "        SCC1_values[tm, path] = SCC1_func(hists[tm,:,path])\n",
    "        SCC2_base_values[tm, path] = SCC2_base_func(hists[tm,:,path]) \n",
    "        SCC2_tilt_values[tm, path] = SCC2_tilt_func(hists[tm,:,path])\n",
    "        SCC2_base_a_values[tm, path] = SCC2_base_a_func(hists[tm,:,path])\n",
    "        \n",
    "SCC_total = np.mean(SCC_values,axis = 1)\n",
    "SCC_private = np.mean(SCC1_values,axis = 1)\n",
    "SCC2_FK_base = np.mean(SCC2_base_values,axis = 1)\n",
    "SCC2_FK_tilt = np.mean(SCC2_tilt_values,axis = 1)\n",
    "\n",
    "uncertain = SCC2_FK_tilt - SCC2_FK_base\n",
    "\n",
    "uncertain = SCC2_FK_tilt - SCC2_FK_base\n",
    "\n",
    "SCCs = {}\n",
    "SCCs['SCC'] = SCC_total   # Total SCC\n",
    "SCCs['SCC1'] = SCC_private   # private SCC\n",
    "SCCs['SCC2'] = SCC2_FK_base   # external SCC\n",
    "SCCs['SCC3'] = SCC2_FK_tilt - SCC2_FK_base  # Uncertainty SCC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark Convergence criteria for Feyman Kac\n",
    "\n",
    "Below table listed the convergence criteria and final errors for solving HJB equations at different growth damages setting which we used in the paper. As the discretized state space contains over 3 million points, even the Conjugate Gradient approximation is quite slow to converge to a small error. Therefore, we cap the max iterations for the conjugate gradient solver to be 400,000.\n",
    "\n",
    "| ambiguity    |   damage specification   | iterations | CG Error |  PDE Error | Approx time\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|averse | base  | 340,113 | 1e-14 | 4.50e-6|2 hrs\n",
    "|averse | worst | 391,635 | 1e-14| 2.21e-6| 2.5 hrs\n",
    "|neutral | base  | 577,167 | 1e-14 | 2.21e-6|3.5 hrs\n",
    "|neutral | worst | 556,956 | 1e-14 | 2.21e-6|3.5 hrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we caculated and plotted the ${\\widetilde q}_2(\\beta)$ implied Ambiguity-Adjusted Probabilities for 9 growth damage modesls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T20:19:18.842575Z",
     "start_time": "2019-12-18T20:19:17.630884Z"
    }
   },
   "outputs": [],
   "source": [
    "Dists = {}\n",
    "\n",
    "a = Œ≤ùòß - 5 * np.sqrt(œÉ·µ¶)\n",
    "b = Œ≤ùòß + 5 * np.sqrt(œÉ·µ¶)\n",
    "a_10std = Œ≤ùòß - 10 * np.sqrt(œÉ·µ¶)\n",
    "b_10std = Œ≤ùòß + 10 * np.sqrt(œÉ·µ¶)\n",
    "beta_f_space = np.linspace(a_10std,b_10std,200)\n",
    "\n",
    "original_dist = norm.pdf(beta_f_space, Œ≤ùòß, np.sqrt(œÉ·µ¶))\n",
    "Dists['Original'] = original_dist\n",
    "\n",
    "R_value = np.mean(hists[:,0,:], axis = 1)\n",
    "K_value = np.mean(hists[:,1,:], axis = 1)\n",
    "F_value = np.mean(hists[:,2,:], axis = 1)\n",
    "\n",
    "R_func_r = []\n",
    "R_func = []\n",
    "Œ≤ÃÉ_func_r = []\n",
    "Œ≤ÃÉ_func = []\n",
    "ŒªÃÉ_func_r = []\n",
    "ŒªÃÉ_func = []\n",
    "for ite in range(len(R_)):\n",
    "    R_func_r.append(GridInterp(gridpoints, R_[ite], method))\n",
    "    R_func.append(lambda x, ite = ite: R_func_r[ite].get_value(np.log(x[0]), x[2], np.log(x[1])))\n",
    "    Œ≤ÃÉ_func_r.append(GridInterp(gridpoints, Œ≤ÃÉ_[ite], method))\n",
    "    Œ≤ÃÉ_func.append(lambda x, ite = ite: Œ≤ÃÉ_func_r[ite].get_value(np.log(x[0]), x[2], np.log(x[1])))\n",
    "    ŒªÃÉ_func_r.append(GridInterp(gridpoints, ŒªÃÉ_[ite], method))\n",
    "    ŒªÃÉ_func.append(lambda x, ite = ite: ŒªÃÉ_func_r[ite].get_value(np.log(x[0]), x[2], np.log(x[1])))\n",
    "\n",
    "hists_mean = np.mean(hists, axis = 2)\n",
    "RE_plot = np.zeros(pers)\n",
    "weight_plot = [np.zeros([pers,1]) for ite in range(len(œÄÃÉ_norm_func))]\n",
    "for tm in range(pers):\n",
    "    RE_plot[tm] = RE_func(hists_mean[tm,:])\n",
    "    for ite in range(len(œÄÃÉ_norm_func)):\n",
    "        weight_plot[ite][tm] = œÄÃÉ_norm_func[ite](hists_mean[tm,:])\n",
    "\n",
    "for tm in [1,100,200,300,400]:\n",
    "    R0 = hists[tm-1,0,0]\n",
    "    K0 = hists[tm-1,1,0]\n",
    "    F0 = hists[tm-1,2,0]\n",
    "    weights_prob = []\n",
    "    mean_distort_ = []\n",
    "    lambda_tilde_ = []\n",
    "    tilt_dist_ = []\n",
    "    \n",
    "    for ite in range(len(œÄÃÉ_norm_func)):\n",
    "        weights_prob.append(œÄÃÉ_norm_func[ite]([R0, K0, F0]))\n",
    "        mean_distort_.append(Œ≤ÃÉ_func[ite]([R0, K0, F0]) - Œ≤f)\n",
    "        lambda_tilde_.append(ŒªÃÉ_func[ite]([R0, K0, F0]))\n",
    "        tilt_dist_.append(norm.pdf(beta_f_space, mean_distort_[ite] + Œ≤f, 1 / np.sqrt(lambda_tilde_[ite])))\n",
    "        \n",
    "    weighted = sum(w * til for w, til in zip(weights_prob, tilt_dist_) )\n",
    "    Dists['Year' + str(int((tm) / 4))] = dict(tilt_dist = tilt_dist_, weighted = weighted, weights = weights_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worst case probabilities for 9 growth damage models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T20:19:20.215180Z",
     "start_time": "2019-12-18T20:19:18.845166Z"
    }
   },
   "outputs": [],
   "source": [
    "growthdensityPlot(beta_f_space, Dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Social Cost of Carbon Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T20:19:20.475916Z",
     "start_time": "2019-12-18T20:19:20.216134Z"
    }
   },
   "outputs": [],
   "source": [
    "growthSCCDecomposePlot(SCCs, Œæp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emission trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T20:19:20.828002Z",
     "start_time": "2019-12-18T20:19:20.476914Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "growthemissionPlot(Œæp, e_hists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 460.989666,
   "position": {
    "height": "482.99px",
    "left": "1359px",
    "right": "20px",
    "top": "183px",
    "width": "324px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
