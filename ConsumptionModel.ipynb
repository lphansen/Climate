{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pricing Uncertainty Induced by Climate Change\n",
    "by [Michael Barnett](https://sites.google.com/site/michaelduglasbarnett/home), [William Brock](https://www.ssc.wisc.edu/~wbrock/) and [Lars Peter Hansen](https://larspeterhansen.org/).\n",
    "\n",
    "The latest draft of the paper can be found [here](https://larspeterhansen.org/research/papers/).\n",
    "\n",
    "Notebook by: Jiaming Wang\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides the source code and explanations for how we solve the model setting with __climate damages to consumption__. Users can find computational details for the model setting with __climate damages to growth__ in the notebook for the [Growth Damages Model](GrowthModel.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Overview</a></span></li><li><span><a href=\"#Code-and-Illustration\" data-toc-modified-id=\"Code-and-Illustration-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Code and Illustration</a></span><ul class=\"toc-item\"><li><span><a href=\"#Choosing-key-parameters\" data-toc-modified-id=\"Choosing-key-parameters-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Choosing key parameters</a></span></li><li><span><a href=\"#Solving-the-HJB-equation-with-consumption-damages\" data-toc-modified-id=\"Solving-the-HJB-equation-with-consumption-damages-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Solving the HJB equation with consumption damages</a></span><ul class=\"toc-item\"><li><span><a href=\"#Remark:-Choices-of-model-hyper-parameters\" data-toc-modified-id=\"Remark:-Choices-of-model-hyper-parameters-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Remark: Choices of model hyper parameters</a></span></li><li><span><a href=\"#Remark:-Discretization-of-state-spaces\" data-toc-modified-id=\"Remark:-Discretization-of-state-spaces-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Remark: Discretization of state spaces</a></span></li><li><span><a href=\"#Remark:-Accounting-for-uncertainty-about-consumption-damages\" data-toc-modified-id=\"Remark:-Accounting-for-uncertainty-about-consumption-damages-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>Remark: Accounting for uncertainty about consumption damages</a></span><ul class=\"toc-item\"><li><span><a href=\"#Low-damage-model\" data-toc-modified-id=\"Low-damage-model-2.2.3.1\"><span class=\"toc-item-num\">2.2.3.1&nbsp;&nbsp;</span>Low damage model</a></span></li><li><span><a href=\"#High-damage-model\" data-toc-modified-id=\"High-damage-model-2.2.3.2\"><span class=\"toc-item-num\">2.2.3.2&nbsp;&nbsp;</span>High damage model</a></span></li><li><span><a href=\"#Distorted-model-probabilities\" data-toc-modified-id=\"Distorted-model-probabilities-2.2.3.3\"><span class=\"toc-item-num\">2.2.3.3&nbsp;&nbsp;</span>Distorted model probabilities</a></span></li><li><span><a href=\"#Weighted-damage-models\" data-toc-modified-id=\"Weighted-damage-models-2.2.3.4\"><span class=\"toc-item-num\">2.2.3.4&nbsp;&nbsp;</span>Weighted damage models</a></span></li></ul></li><li><span><a href=\"#Remark:--Details-for-solving-HJB-PDE\" data-toc-modified-id=\"Remark:--Details-for-solving-HJB-PDE-2.2.4\"><span class=\"toc-item-num\">2.2.4&nbsp;&nbsp;</span>Remark:  Details for solving HJB PDE</a></span></li><li><span><a href=\"#Remark:-Conjugate-gradient-solver-for-linear-system\" data-toc-modified-id=\"Remark:-Conjugate-gradient-solver-for-linear-system-2.2.5\"><span class=\"toc-item-num\">2.2.5&nbsp;&nbsp;</span>Remark: Conjugate gradient solver for linear system</a></span></li><li><span><a href=\"#Remark:-PDE-boundary-conditions\" data-toc-modified-id=\"Remark:-PDE-boundary-conditions-2.2.6\"><span class=\"toc-item-num\">2.2.6&nbsp;&nbsp;</span>Remark: PDE boundary conditions</a></span></li><li><span><a href=\"#Remark:-Tolerance-level-and-conergence-criteria-for-HJB\" data-toc-modified-id=\"Remark:-Tolerance-level-and-conergence-criteria-for-HJB-2.2.7\"><span class=\"toc-item-num\">2.2.7&nbsp;&nbsp;</span>Remark: Tolerance level and conergence criteria for HJB</a></span></li><li><span><a href=\"#Remark:-Solving-time-and-error-analysis\" data-toc-modified-id=\"Remark:-Solving-time-and-error-analysis-2.2.8\"><span class=\"toc-item-num\">2.2.8&nbsp;&nbsp;</span>Remark: Solving time and error analysis</a></span></li></ul></li><li><span><a href=\"#Simulation\" data-toc-modified-id=\"Simulation-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Simulation</a></span></li><li><span><a href=\"#SCC-Calculation-Feyman-Kac\" data-toc-modified-id=\"SCC-Calculation-Feyman-Kac-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>SCC Calculation Feyman Kac</a></span><ul class=\"toc-item\"><li><span><a href=\"#Remark:-Convergence-criteria-for-Feyman-Kac\" data-toc-modified-id=\"Remark:-Convergence-criteria-for-Feyman-Kac-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>Remark: Convergence criteria for Feyman Kac</a></span></li></ul></li><li><span><a href=\"#Probabilities\" data-toc-modified-id=\"Probabilities-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Probabilities</a></span></li></ul></li><li><span><a href=\"#Results\" data-toc-modified-id=\"Results-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Results</a></span><ul class=\"toc-item\"><li><span><a href=\"#Implied-worst-case-densities\" data-toc-modified-id=\"Implied-worst-case-densities-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Implied worst case densities</a></span></li><li><span><a href=\"#SCC-Decomposition\" data-toc-modified-id=\"SCC-Decomposition-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>SCC Decomposition</a></span></li><li><span><a href=\"#Emission-trajectory\" data-toc-modified-id=\"Emission-trajectory-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Emission trajectory</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code and Illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T07:43:01.250181Z",
     "start_time": "2021-01-26T07:42:56.401537Z"
    }
   },
   "outputs": [],
   "source": [
    "# Required packages\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('./src')\n",
    "from supportfunctions import *\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing key parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T07:43:01.262089Z",
     "start_time": "2021-01-26T07:43:01.252114Z"
    }
   },
   "outputs": [],
   "source": [
    "# Damage function choices\n",
    "damageSpec = 'Weighted'  # Choose among \"High\"(Weitzman), 'Low'(Nordhaus) and 'Weighted' (arithmeticAverage of the two)\n",
    "\n",
    "if damageSpec == 'High':\n",
    "    weight = 0.0\n",
    "elif damageSpec == 'Low':\n",
    "    weight = 1.0\n",
    "else:\n",
    "    weight = 0.5\n",
    "\n",
    "Œæp =  1 / 4000  # Ambiguity Averse Paramter \n",
    "# We stored solutions for Œæp =  1 / 4000 to which we referred as \"Ambiguity Averse\" and Œæp = 1000 as ‚ÄúAmbiguity Neutral‚Äù in the paper\n",
    "# Sensible choices are from 0.0002 to 4000, while for parameters input over 0.01 the final results won't alter as much\n",
    "\n",
    "if Œæp < 1:\n",
    "    aversespec = \"Averse\"\n",
    "else:\n",
    "    aversespec = 'Neutral'\n",
    "\n",
    "smart_guess = False\n",
    "model = 'model1'\n",
    "filename = 'BBH_' + model + '_' + damageSpec + '_' + aversespec\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T07:43:01.269071Z",
     "start_time": "2021-01-26T07:43:01.264083Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Loading Smart guesses\n",
    "# # Do Not run this cell if you don't want to use smart guesses, it might take longer than 18 hours. See remarks in section 2.2.8\n",
    "# guess = pickle.load(open('./data/{}guess.pickle'.format(damageSpec + aversespec), \"rb\", -1))\n",
    "# v0_guess = guess['v0']\n",
    "# q_guess = guess['q']\n",
    "# e_guess = guess['e']\n",
    "# # base_guess = guess['base']\n",
    "# # worst_guess = guess['worst']\n",
    "# smart_guess = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the HJB equation with consumption damages\n",
    "\n",
    "We now outline the code used to solve the HJB equation for the damages to consumption model. We summarize the steps for the numerical algorithm and refer to those steps in the code below. More details are provided in the remarks.\n",
    "\n",
    "To solve the nonlinear partial differential equations that characterize the HJB equations for the planner's problems for our model, we use a so-called implicit, finite-difference scheme and a conjugate gradient method. Consultations with Joseph Huang, Paymon Khorrami and Fabrice Tourre played an important role in the software implementation. We briefly outline the steps to this numerical solution method below.\n",
    "\n",
    "Recall that the HJB equation of interest for the consumption damages model includes both minimization and maximization:\n",
    "\n",
    "\\begin{align*} 0 = \\max_{a \\in {\\mathbb A}} \\min_{q > 0, \\int q P(d\\theta) =1 } \\min_{g \\in {\\mathbb R}^m} & - \\delta V(x)  + \\delta (1 - \\kappa) \\left[ \\log \\left( {\\alpha}  - i - j \\right)\n",
    "+ k -  d   \\right] + \\delta \\kappa \\left( \\log e +  r \\right) \\cr &\n",
    "+ {\\frac {\\partial V}{\\partial x}} (x) \\cdot \\left[\\int_\\Theta  \\mu_X(x,a \\mid \\theta) q(\\theta) P(d\\theta)  + \\sigma_X(x) g\\right] \\cr &\n",
    "+{\\frac 1 2} \\textrm{trace} \\left[\\sigma_X(x)' {\\frac {\\partial^2 V}{\\partial x \\partial x'}}(x) \\sigma_X(x) \\right] \\cr &\n",
    "+ {\\frac {\\xi_m} 2} g'g + \\xi_p \\int_\\Theta [\\log q(\\theta)]  q(\\theta) P(d \\theta).\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed recursively as follows:\n",
    "\n",
    "\n",
    "1) start with a value function guess ${\\widetilde V}(x)$ and a decision function ${\\widetilde a}(x)$;\n",
    "\n",
    "2) given $({\\widetilde V}, {\\widetilde a})$, solve the minimization problem embedded in the HJB equation and produce an exponentially-tilted  density ${\\widehat  q}$ and drift distortion ${\\widehat g}$  conditioned on $x$ and  using the  approach described   in section D of [Online Appendix](http://larspeterhansen.org/wp-content/uploads/2020/01/RFSPaper_BBH_final_appendix.pdf);\n",
    "\n",
    "3) compute the implied relative entropy from the change in prior:\n",
    "$$\n",
    "{\\widehat {\\mathbb I}}(x) = \\int_\\Theta [\\log {\\widehat q}(\\theta)]  {\\widehat q}(\\theta) P(d \\theta);\n",
    "$$\n",
    "\n",
    "4)  solve the following maximization problem by choice of $a=(i,j,e)$:\n",
    "\n",
    "\\begin{align*}\n",
    "& \\delta (1 - \\kappa)  \\log \\left( {\\alpha}  - i - j \\right)\n",
    " + \\delta \\kappa  \\log e   \\cr &\n",
    "+ {\\frac {\\partial V}{\\partial x}} (x) \\cdot \\int_\\Theta  \\mu_X\\left(x,a   \\mid \\theta \\right) {\\widehat q}(\\theta \\mid x ) P(d\\theta);\n",
    "\\end{align*}\n",
    "\n",
    "   a) Compute ${\\hat i}$ and ${\\hat j}$ by solving the two first-order conditions for $i$ and $j$ with cobweb-style iterations.  Cobweb iterations converge or diverge depending the relative slopes of supply and demand functions.  By shrinking the step size, these slopes can be altered.\n",
    "\n",
    "Expand the two equation system by adding a third equation that defines  a common \"price\" $p$,\n",
    "$$\n",
    "p =  {\\frac {\\delta (1-\\kappa)}  {\\alpha  - i - j} } = g(i+j).\n",
    "$$\n",
    "Write the two first-order conditions as\n",
    "$$\n",
    "p = {\\frac {\\phi_0 \\phi_1 V_k(x) }{ 1 + \\phi_1 i}} = f_1(i)\n",
    "$$\n",
    "$$\n",
    "p  = V_r(x)  \\left(  {\\psi_0 \\psi_1} \\right) j^{\\psi_1 - 1}  \\exp\\left[ \\psi_1(k -  r)\\right]  = f_2(j).\n",
    "$$\n",
    "\n",
    "Given $p$  and for step size $\\tilde{\\epsilon}$, compute\n",
    "\n",
    "* $i^* = (f_1)^{-1}(p)$\n",
    "\n",
    "* $j^* = (f_2)^{-1}(p)$\n",
    "\n",
    "* $p^* = {\\eta} g(i^* + j^*) + \\left(1 - {\\eta} \\right) p$\n",
    "\n",
    "* set $p  = p^*$.\n",
    "\n",
    "Iterate to convergence.\n",
    "\n",
    "\n",
    "b) Compute ${\\hat e}$ by solving the first-order conditions\n",
    "$$\n",
    "{\\frac {\\delta \\kappa} e} +  {\\frac d {d e}} \\left[V_x(x)  \\cdot \\int_\\Theta  \\mu_X\\left(x, i, j, a  \\mid \\theta \\right) {\\widehat q}(\\theta \\mid x ) P(d\\theta)\\right] = 0 .\n",
    "$$\n",
    "These first-order conditions turn out not to depend on $(i,j)$.\n",
    "\n",
    "\n",
    "5) use the minimization output from step (2) and  maximization output from step (4) and construct an adjusted drift using the following formula, which is the analog to formula (20) from the paper:\n",
    "\\begin{equation*}\n",
    "{\\widehat \\mu}(x) = \\int_\\Theta  \\mu_X\\left(x, {\\widehat a}  \\mid \\theta \\right) {\\widehat q}(\\theta \\mid x ) P(d\\theta) + \\sigma_X(x) {\\widehat g}(x);\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "6)  construct the linear equation system for a new value function $V = {\\widehat V}$:\n",
    "\n",
    "\\begin{align*}\n",
    "0 =  & - \\delta V(x)  + \\delta (1 - \\kappa) \\left( \\log \\left[ {\\alpha}  - {\\widehat i}(x)  - {\\widehat j} (x)  \\right]\n",
    "+ k -  d   \\right) + \\delta \\kappa \\left[ \\log {\\widehat e}(x)   +  r \\right] \\cr\n",
    "& + {\\frac {\\partial V}{\\partial x}} (x) \\cdot {\\widehat \\mu}(x)\n",
    "+{\\frac 1 2} \\textrm{trace} \\left[\\sigma_X(x)' {\\frac {\\partial^2 V}{\\partial x \\partial x'}}(x) \\sigma_X(x) \\right] \\cr\n",
    "& + {\\frac {\\xi_m} 2} {\\widehat g}(x) \\cdot {\\widehat g}(x)   + \\xi_p {\\widehat {\\mathbb I}}(x);\n",
    "\\end{align*}\n",
    "\n",
    "7) modify this equation by adding a so-called \"false transient\" to the left-hand side:\n",
    "\n",
    "\\begin{align*}\n",
    "{\\frac {V(x) - {\\widetilde V}(x)} \\epsilon}  =  & - \\delta V(x)  + \\delta (1 - \\kappa) \\left( \\log \\left[ {\\alpha}  - {\\widehat i}(x)  - {\\widehat j} (x)  \\right]\n",
    "+ k -  d    \\right) + \\delta \\kappa \\left[ \\log {\\widehat e}(x)   +  r \\right] \\cr\n",
    "& + {\\frac {\\partial V}{\\partial x}} (x) \\cdot {\\widehat \\mu}(x)\n",
    "+{\\frac 1 2} \\textrm{trace} \\left[\\sigma_X(x)' {\\frac {\\partial^2 V}{\\partial x \\partial x'}}(x) \\sigma_X(x) \\right] \\cr\n",
    "& + {\\frac {\\xi_m} 2} {\\widehat g}(x) \\cdot {\\widehat g}(x)   + \\xi_p {\\widehat {\\mathbb I}}(x);\n",
    "\\end{align*}\n",
    "\n",
    "8) solve the linear system from step (7) for $V= {\\widehat V}$ using a conjugate-gradient method;\n",
    "\n",
    "9) set ${\\widetilde V} = {\\widehat V}$ and ${\\widetilde a} = {\\widehat a}$ and repeat steps (2) - (8)  until convergence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T07:45:17.393046Z",
     "start_time": "2021-01-26T07:43:01.271065Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "    \n",
    "McD = np.loadtxt('./data/TCRE_MacDougallEtAl2017_update.txt')\n",
    "par_lambda_McD = McD / 1000\n",
    "\n",
    "Œ≤ùòß = np.mean(par_lambda_McD)  # Climate sensitivity parameter, MacDougall (2017)\n",
    "œÉ·µ¶ = np.var(par_lambda_McD, ddof = 1)  # varaiance of climate sensitivity parameters\n",
    "Œª = 1.0 / œÉ·µ¶ \n",
    "\n",
    "# Parameters as defined in the paper\n",
    "Œ¥ = 0.01        \n",
    "Œ∫ = 0.032       \n",
    "œÉùò® = 0.02\n",
    "œÉùò¨ = 0.0161\n",
    "œÉùò≥ = 0.0339 \n",
    "Œ± = 0.115000000000000\n",
    "œï0 = 0.0600\n",
    "œï1 = 16.666666666666668\n",
    "Œºk = -0.034977443912449\n",
    "œà0 = 0.112733407891680\n",
    "œà1 = 0.142857142857143\n",
    "\n",
    "# parameters for damage function settings\n",
    "power = 2 \n",
    "Œ≥1 = 0.00017675\n",
    "Œ≥2 = 2. * 0.0022\n",
    "Œ≥2_plus = 2. * 0.0197\n",
    "Œ≥ÃÑ2_plus = weight * 0 + (1 - weight) * Œ≥2_plus\n",
    "\n",
    "œÉ1 = 0\n",
    "œÉ2 = 0\n",
    "œÅ12 = 0\n",
    "FÃÑ = 2\n",
    "crit = 2\n",
    "F0 = 1\n",
    "\n",
    "xi_d = -1 * (1 - Œ∫)\n",
    "\n",
    "# See Remark 2.1.1 regarding the choice of Œµ and Œ∑\n",
    "# False Trasient Time step\n",
    "Œµ = 0.3\n",
    "# Cobweb learning rate\n",
    "Œ∑ = 0.05\n",
    "\n",
    "\n",
    "# Specifying Tolerance level\n",
    "tol = 1e-8\n",
    "\n",
    "# Grids Specification\n",
    "\n",
    "# Coarse Grids\n",
    "# nR = 30\n",
    "# nF = 40\n",
    "# nK = 25\n",
    "# R = np.linspace(R_min, R_max, nR)\n",
    "# F = np.linspace(F_min, F_max, nF)\n",
    "# K = np.linspace(K_min, K_max, nK)\n",
    "\n",
    "# hR = R[1] - R[0]\n",
    "# hF = F[1] - F[0]\n",
    "# hK = K[1] - K[0]\n",
    "\n",
    "# Dense Grids\n",
    "\n",
    "R_min = 0\n",
    "R_max = 9\n",
    "F_min = 0\n",
    "F_max = 4000\n",
    "K_min = 0\n",
    "K_max = 18\n",
    "\n",
    "hR = 0.05\n",
    "hF = 25\n",
    "hK = 0.15\n",
    "\n",
    "R = np.arange(R_min, R_max + hR, hR)\n",
    "nR = len(R)\n",
    "F = np.arange(F_min, F_max + hF, hF)\n",
    "nF = len(F)\n",
    "K = np.arange(K_min, K_max + hK, hK)\n",
    "nK = len(K)\n",
    "\n",
    "# Discretization of the state space for numerical PDE solution. \n",
    "# See Remark 2.1.2\n",
    "(R_mat, F_mat, K_mat) = np.meshgrid(R,F,K, indexing = 'ij')\n",
    "stateSpace = np.hstack([R_mat.reshape(-1,1,order = 'F'),F_mat.reshape(-1,1,order = 'F'),K_mat.reshape(-1,1,order = 'F')])\n",
    "\n",
    "# Inputs for function quad_int\n",
    "# Integrating across parameter distribution\n",
    "quadrature = 'legendre'\n",
    "n = 30\n",
    "a = Œ≤ùòß - 5 * np.sqrt(œÉ·µ¶)\n",
    "b = Œ≤ùòß + 5 * np.sqrt(œÉ·µ¶)\n",
    "\n",
    "v0 = Œ∫ * R_mat + (1-Œ∫) * K_mat - Œ≤ùòß * F_mat\n",
    "\n",
    "FC_Err = 1\n",
    "episode = 0\n",
    "\n",
    "if smart_guess:\n",
    "    v0 = v0_guess\n",
    "    q = q_guess\n",
    "    e_star = e_guess\n",
    "    episode = 1\n",
    "    Œµ = 0.2\n",
    "    \n",
    "while episode == 0 or FC_Err > tol:\n",
    "    start_time1 = time.time()\n",
    "    vold = v0.copy()\n",
    "    # Applying finite difference scheme to the value function\n",
    "    v0_dr = finiteDiff(v0,0,1,hR) \n",
    "    v0_df = finiteDiff(v0,1,1,hF)\n",
    "    v0_dk = finiteDiff(v0,2,1,hK)\n",
    "\n",
    "    v0_drr = finiteDiff(v0,0,2,hR)\n",
    "    v0_drr[v0_dr < 1e-16] = 0\n",
    "    v0_dr[v0_dr < 1e-16] = 1e-16\n",
    "    v0_dff = finiteDiff(v0,1,2,hF)\n",
    "    v0_dkk = finiteDiff(v0,2,2,hK)\n",
    "    if episode > 2000:\n",
    "        Œµ = 0.1\n",
    "    elif episode > 1000:\n",
    "        Œµ = 0.2\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    if episode == 0:\n",
    "        # First time into the loop\n",
    "        B1 = v0_dr - xi_d * (Œ≥1 + Œ≥2 * F_mat * Œ≤ùòß + Œ≥2_plus * (F_mat * Œ≤ùòß - FÃÑ) ** (power - 1) * (F_mat >= (crit / Œ≤ùòß))) * Œ≤ùòß * np.exp(R_mat) - v0_df * np.exp(R_mat)\n",
    "        C1 = - Œ¥ * Œ∫\n",
    "        e = -C1 / B1\n",
    "        e_hat = e\n",
    "        Acoeff = np.ones(R_mat.shape)\n",
    "        Bcoeff = ((Œ¥ * (1 - Œ∫) * œï1 + œï0 * œï1 * v0_dk) * Œ¥ * (1 - Œ∫) / (v0_dr * œà0 * 0.5) * np.exp(0.5 * (R_mat - K_mat))) / (Œ¥ * (1 - Œ∫) * œï1)\n",
    "        Ccoeff = -Œ±  - 1 / œï1\n",
    "        j = ((-Bcoeff + np.sqrt(Bcoeff ** 2 - 4 * Acoeff * Ccoeff)) / (2 * Acoeff)) ** 2\n",
    "        i = Œ± - j - (Œ¥ * (1 - Œ∫)) / (v0_dr * œà0 * 0.5) * j ** 0.5 * np.exp(0.5 * (R_mat - K_mat))\n",
    "        q = Œ¥ * (1 - Œ∫) / (Œ± - i - j)\n",
    "    else:\n",
    "        e_hat = e_star\n",
    "        \n",
    "        # Step 4 (a) : Cobeweb scheme to update controls i and j; q is an intermediary variable that determines i and j\n",
    "        Converged = 0\n",
    "        nums = 0\n",
    "        while Converged == 0:\n",
    "            i_star = (œï0 * œï1 * v0_dk / q - 1) / œï1\n",
    "            j_star = (q * np.exp(œà1 * (R_mat - K_mat)) / (v0_dr * œà0 * œà1)) ** (1 / (œà1 - 1))\n",
    "            if Œ± > np.max(i_star + j_star):\n",
    "                q_star = Œ∑ * Œ¥ * (1 - Œ∫) / (Œ± - i_star - j_star) + (1 - Œ∑) * q\n",
    "            else:\n",
    "                q_star = 2 * q\n",
    "            if np.max(abs(q - q_star) / Œ∑) <= 1e-5:\n",
    "                Converged = 1\n",
    "                q = q_star\n",
    "                i = i_star\n",
    "                j = j_star\n",
    "            else:\n",
    "                q = q_star\n",
    "                i = i_star\n",
    "                j = j_star\n",
    "            \n",
    "            nums += 1\n",
    "        if episode % 100 == 0:\n",
    "            print('Cobweb Passed, iterations: {}, i error: {:10f}, j error: {:10f}'.format(nums, np.max(i - i_star), np.max(j - j_star)))\n",
    "\n",
    "    a1 = np.zeros(R_mat.shape)\n",
    "    b1 = xi_d * e_hat * np.exp(R_mat) * Œ≥1\n",
    "    c1 = 2 * xi_d * e_hat * np.exp(R_mat) * F_mat * Œ≥2 \n",
    "    ŒªÃÉ1 = Œª + c1 / Œæp\n",
    "    Œ≤ÃÉ1 = Œ≤ùòß - c1 * Œ≤ùòß / (Œæp * ŒªÃÉ1) -  b1 /  (Œæp * ŒªÃÉ1)\n",
    "    I1 = a1 - 0.5 * np.log(Œª) * Œæp + 0.5 * np.log(ŒªÃÉ1) * Œæp + 0.5 * Œª * Œ≤ùòß ** 2 * Œæp - 0.5 * ŒªÃÉ1 * (Œ≤ÃÉ1) ** 2 * Œæp\n",
    "    R1 = 1 / Œæp * (I1 - (a1 + b1 * Œ≤ÃÉ1 + c1 / 2 * Œ≤ÃÉ1 ** 2 + c1 / 2 / ŒªÃÉ1))\n",
    "    J1_without_e = xi_d * (Œ≥1 * Œ≤ÃÉ1 + Œ≥2 * F_mat * (Œ≤ÃÉ1 ** 2 + 1 / ŒªÃÉ1)) * np.exp(R_mat)\n",
    "\n",
    "    œÄÃÉ1 = weight * np.exp(-1 / Œæp * I1)\n",
    "\n",
    "    # Step (2), solve minimization problem in HJB and calculate drift distortion\n",
    "    # See remark 2.1.3 for more details\n",
    "    start_time2 = time.time()\n",
    "    if episode == 0 or (smart_guess and episode == 1):\n",
    "        @nb.jit(nopython = True, parallel = True)\n",
    "        def scale_2_fnc(x, e_hat):\n",
    "            return np.exp(-1 / Œæp * xi_d * (Œ≥1 * x + Œ≥2 * x ** 2 * F_mat + Œ≥2_plus * x * (x * F_mat - FÃÑ) ** (power - 1) * ((x * F_mat - FÃÑ) >= 0)) * np.exp(R_mat) * e_hat)  * normpdf(x,Œ≤ùòß,np.sqrt(œÉ·µ¶))\n",
    "\n",
    "        @nb.jit(nopython = True, parallel = True)\n",
    "        def q2_tilde_fnc(x, e_hat, scale_2):\n",
    "            return np.exp(-1 / Œæp * xi_d * (Œ≥1 * x + Œ≥2 * x ** 2 * F_mat + Œ≥2_plus * x * (x * F_mat - FÃÑ) ** (power - 1) * ((x * F_mat - FÃÑ) >= 0)) * np.exp(R_mat) * e_hat) / scale_2\n",
    "\n",
    "        @nb.jit(nopython = True, parallel = True)\n",
    "        def J2_without_e_fnc(x, e_hat, scale_2):\n",
    "            return xi_d * np.exp(R_mat) * q2_tilde_fnc(x, e_hat, scale_2) * (Œ≥1 * x + Œ≥2 * F_mat * x ** 2 + Œ≥2_plus * x * (x * F_mat - FÃÑ) ** (power - 1) * ((x * F_mat - FÃÑ) >= 0)) * normpdf(x,Œ≤ùòß,np.sqrt(œÉ·µ¶))\n",
    "        \n",
    "        (xs,ws) = quad_points_legendre(n)\n",
    "        xs = (b-a) * 0.5  * xs + (a + b) * 0.5\n",
    "        s = np.prod((b-a) * 0.5)\n",
    "    \n",
    "    scale_2 = np.zeros(F_mat.shape)\n",
    "    for i_iter in range(n):\n",
    "        scale_2 += ws[i_iter] * scale_2_fnc(xs[i_iter], e_hat)\n",
    "    scale_2 = s * scale_2\n",
    "    \n",
    "    I2 = -1 * Œæp * np.log(scale_2)\n",
    "\n",
    "    J2_without_e = np.zeros(F_mat.shape)\n",
    "    for i_iter in range(n):\n",
    "        J2_without_e += ws[i_iter] * J2_without_e_fnc(xs[i_iter], e_hat, scale_2)\n",
    "    J2_without_e = s * J2_without_e\n",
    "    J2_with_e = J2_without_e * e_hat\n",
    "    end_time2 = time.time()\n",
    "\n",
    "    R2 = (I2 - J2_with_e) / Œæp\n",
    "    œÄÃÉ2 = (1 - weight) * np.exp(-1 / Œæp * I2)\n",
    "    œÄÃÉ1_norm = œÄÃÉ1 / (œÄÃÉ1 + œÄÃÉ2)\n",
    "    œÄÃÉ2_norm = 1 - œÄÃÉ1_norm\n",
    "\n",
    "    # step 4 (b) updating e based on first order conditions\n",
    "    expec_e_sum = (œÄÃÉ1_norm * J1_without_e + œÄÃÉ2_norm * J2_without_e)\n",
    "\n",
    "    B1 = v0_dr - v0_df * np.exp(R_mat) - expec_e_sum\n",
    "    C1 = -Œ¥ * Œ∫\n",
    "    e = -C1 / B1\n",
    "    e_star = e\n",
    "\n",
    "    J1 = J1_without_e * e_star\n",
    "    J2 = J2_without_e * e_star\n",
    "\n",
    "    # Step (3) calculating implied entropies\n",
    "    I_term = -1 * Œæp * np.log(œÄÃÉ1 + œÄÃÉ2)\n",
    "\n",
    "    R1 = (I1 - J1) / Œæp\n",
    "    R2 = (I2 - J2) / Œæp\n",
    "    \n",
    "    # Step (5) solving for adjusted drift\n",
    "    drift_distort = (œÄÃÉ1_norm * J1 + œÄÃÉ2_norm * J2)\n",
    "\n",
    "    if weight == 0 or weight == 1:\n",
    "        RE = œÄÃÉ1_norm * R1 + œÄÃÉ2_norm * R2\n",
    "    else:\n",
    "        RE = œÄÃÉ1_norm * R1 + œÄÃÉ2_norm * R2 + œÄÃÉ1_norm * np.log(\n",
    "            œÄÃÉ1_norm / weight) + œÄÃÉ2_norm * np.log(œÄÃÉ2_norm / (1 - weight))\n",
    "\n",
    "    RE_total = Œæp * RE\n",
    "\n",
    "    # Step (6) and (7) Formulating HJB False Transient parameters\n",
    "    # See remark 2.1.4 for more details\n",
    "    A = -Œ¥ * np.ones(R_mat.shape)\n",
    "    B_r = -e_star + œà0 * (j ** œà1) * np.exp(œà1 * (K_mat - R_mat)) - 0.5 * (œÉùò≥ ** 2)\n",
    "    B_f = e_star * np.exp(R_mat)\n",
    "    B_k = Œºk + œï0 * np.log(1 + i * œï1) - 0.5 * (œÉùò¨ ** 2)\n",
    "    C_rr = 0.5 * œÉùò≥ ** 2 * np.ones(R_mat.shape)\n",
    "    C_ff = np.zeros(R_mat.shape)\n",
    "    C_kk = 0.5 * œÉùò¨ ** 2 * np.ones(R_mat.shape)\n",
    "    D = Œ¥ * Œ∫ * np.log(e_star) + Œ¥ * Œ∫ * R_mat + Œ¥ * (1 - Œ∫) * (np.log(Œ± - i - j) + K_mat) + drift_distort + RE_total # + I_term \n",
    "\n",
    "    # Step (8) solving linear system using a conjugate-gradient method in C++\n",
    "    # See remark 2.1.5, 2.1.6 for more details\n",
    "    start_time3 = time.time()\n",
    "    out = PDESolver(stateSpace, A, B_r, B_f, B_k, C_rr, C_ff, C_kk, D, v0, Œµ, solverType = 'False Transient')\n",
    "    out_comp = out[2].reshape(v0.shape,order = \"F\")\n",
    "    \n",
    "    # Calculating PDE error and False Transient error\n",
    "    PDE_rhs = A * v0 + B_r * v0_dr + B_f * v0_df + B_k * v0_dk + C_rr * v0_drr + C_kk * v0_dkk + C_ff * v0_dff + D\n",
    "    PDE_Err = np.max(abs(PDE_rhs))\n",
    "    FC_Err = np.max(abs((out_comp - v0)))\n",
    "    print(\"Total: {:.2f} seconds; Quadrature: {:.2f} seconds; Conjugate Gradient: {:.2f} seconds\".format(time.time() - start_time1, end_time2 - start_time2, time.time() - start_time3))\n",
    "    if episode % 1 == 0:\n",
    "        print(\"Episode {:d}: PDE Error: {:.10f}; False Transient Error: {:.10f}; Iterations: {:d}; CG Error: {:.10f}\" .format(episode, PDE_Err, FC_Err, out[0], out[1]))\n",
    "    episode += 1\n",
    "    \n",
    "    # step 9: keep iterating until convergence\n",
    "    v0 = out_comp\n",
    "\n",
    "print(\"Episode {:d}: PDE Error: {:.10f}; False Transient Error: {:.10f}; Iterations: {:d}; CG Error: {:.10f}\" .format(episode, PDE_Err, FC_Err, out[0], out[1]))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark: Choices of model hyper parameters\n",
    "The choices of ${\\eta}$ in step (4) and $\\epsilon$ in step (7) are made by trading off increases in speed of convergence, achieved by increasing their magnitudes, and enhancing stability of the iterative algorithm, achieved by decreasing their magnitudes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark: Discretization of state spaces\n",
    "We discretize the state space of $x$ using a set number of points along each of the three dimensions and impose a  fixed step size between points for each of these dimensions.  For interior points, we approximate the first derivatives using a first-order upwind scheme while the second derivatives are calculated using a central difference scheme. Upwind schemes are one-sided difference approximations that use the sign of the drifts for the states to determine the direction of the difference.  (See, for instance,  \"An Introduction to Finite Difference Methods for PDE methods in Finance\" by Agnes Tourin, Fields Institute.)  At boundary points we sometimes only have one option used in the approximation. We use a symmetric second difference approximation whenever possible and switch to a one-sided approximation as needed at boundary points.  With this construction, we have reduced the right-hand side of equaton in step (7) as a matrix applied to the value function at the chosen set of discrete points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark: Accounting for uncertainty about consumption damages\n",
    "\n",
    "In calculating consumption damage uncertainty, the term of interest is the contribution of smooth ambiguity to the planner's HJB equation:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "(\\kappa -1) \\left[ \\gamma_1 \\beta + \\gamma_2 \\beta^2 f + \\gamma_2^+\\beta\\left(\\beta f - {\\overline \\gamma} \\right)^2 \\mathbb{1}(\\beta f > {\\overline \\gamma} ) \\right] \\exp(r) e.\n",
    "\\end{eqnarray}\n",
    "\n",
    "Calculating this contibution to the HJB equation depends on a weighted average of two conditional models: the low damage model and the high damage model. The weighting of these conditional contributions is calculated using either the baseline parameter distribution or the ambiguity aversion adjusted or worst-case parameter distribution. \n",
    "\n",
    "##### Low damage model\n",
    "\n",
    "For the low damage model $\\gamma_2^+ = 0$ and so we can derive a quasi-analytical expression for the impact of ambiguity aversion:\n",
    "\n",
    "We begin by computing\n",
    "\\begin{eqnarray}\n",
    "{\\mathcal I}_1  = - \\xi_p \\log E\\left( \\exp\\left[ - {\\frac 1 {\\xi_p}} \\left(  {\\sf a}_1 + {\\sf b}_1 \\beta  + {\\frac {{\\sf c}_1} 2}   \\beta ^2 \\right) \\right] \\mid i \\right),\n",
    "\\end{eqnarray}\n",
    "where\n",
    "\\begin{align*}\n",
    "{\\sf a}_1& = 0,  \\cr\n",
    " {\\sf b}_1   & =  (\\kappa - 1) {\\widehat e} \\exp(r)  \\gamma_1, \\cr\n",
    "{\\sf c}_1   & =  (\\kappa -1) {\\widehat e} \\exp(r)  f \\gamma_2.\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We exploit the exponential quadratic form of the normal density function by __completing-the-square__ to derive our expression. Specifically, we compute the precision for the worst-case distribution by equating the log-quadratic terms from the distributional expression for the altered distribution of $\\beta$\n",
    "\\begin{eqnarray}\n",
    " \\lambda  + {\\frac {{\\sf c}_1}{\\xi_p}} = {\\widetilde \\lambda}_1,\n",
    "\\end{eqnarray}\n",
    "where $\\lambda$ is the precision for the baseline distribution and ${\\widetilde \\lambda}_i$ is the precision for the worst-case distribution.\n",
    "\n",
    "To calculate the worst-case mean, we equate the linear terms from the distributional expression for the altered distribution of $\\beta$\n",
    "\\begin{eqnarray}\n",
    "{\\overline \\beta} \\lambda - {\\frac 1 {\\xi_p}}  {\\sf b}_1 = {\\widetilde \\beta}_1 {\\widetilde \\lambda}_1.\n",
    "\\end{eqnarray}\n",
    "Thus, the worst-case mean is:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "{\\widetilde \\beta}_1  =  {\\overline \\beta}\\left({\\frac {\\lambda}{{\\widetilde \\lambda}_1}} \\right)   - {\\frac 1 {\\xi_p {{\\widetilde \\lambda}_1 }}} {\\sf b}_1  = {\\overline \\beta} -  {\\frac {{\\sf c}_1}{\\xi_p \\widetilde \\lambda_i}}{\\overline \\beta}\n",
    "-  {\\frac 1 {\\xi_p {{\\widetilde \\lambda}_1 }}} {\\sf b}_1\n",
    "\\end{eqnarray}\n",
    "\n",
    "Finally, we compute ${\\mathcal I}_i$ by a) bringing it inside an exponential term,  b) multiplying this by the worst-case normal density that we just deduced, and c) equating the constant terms:\n",
    "\n",
    "\\begin{equation*}\n",
    "- {\\frac 1 {\\xi_p}} {\\mathcal I}_1   +   {\\frac 1 2} \\log {\\widetilde \\lambda}_1 - {\\frac 1   2} \\left({\\widetilde  \\beta}_1 \\right)^2 {\\widetilde \\lambda}_1  = - {\\frac 1 {\\xi_p}} {\\sf a}_1 + {\\frac 1  2} \\log \\lambda - {\\frac 1 2} \\left({\\overline \\beta} \\right)^2 \\lambda .\n",
    "\\end{equation*}\n",
    "\n",
    "Therefore\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "{\\mathcal I}_1 & = & {\\sf a}_1  -   {\\frac {\\xi_p}  2} \\log \\lambda + {\\frac {\\xi_p}  2} \\log {\\widetilde \\lambda}_1\n",
    " + {\\frac {\\xi_p \\lambda}  2} \\left({\\overline \\beta} \\right)^2   - {\\frac {\\xi_p {\\widetilde \\lambda}_1}  2} \\left({\\widetilde  \\beta}_1 \\right)^2, \\cr\n",
    "{\\mathcal R}_1 & = & {\\frac 1 {\\xi_p}} \\left({\\mathcal I}_1 -  \\left[ {\\sf a}_1  + {\\sf b}_1 {\\widetilde \\beta}_1\n",
    "+ {\\sf c}_1  \\left({\\widetilde \\beta}_1 \\right)^2 + {\\frac {{\\sf c}_1} { {\\widetilde \\lambda}_1 }} \\right]  \\right), \\cr\n",
    "{\\mathcal J}_1(e) & = & (\\kappa -1)  \\left( \\gamma_1 {\\widetilde \\beta}_1 + \\gamma_2 \\left[  \\left({\\widetilde \\beta}_1\\right)^2\n",
    "+ {\\frac 1 {\\widetilde \\lambda}_1}\\right]\n",
    "f\\right)  \\exp(r) e,\n",
    "\\end{eqnarray*}\n",
    "\n",
    "where ${\\mathcal I}$ is the smooth damage ambiguity contribution to the planner's HJB equation; ${\\mathcal R }$ measures the relative entropies of the model; and ${\\mathcal J}$ measures the drift distortion altered by the worst-case density due to concerns about ambiguity. \n",
    "\n",
    "##### High damage model\n",
    "\n",
    "We must proceed differently for the high damage model with numerical computation. We begin by calculating\n",
    "\\begin{eqnarray}\n",
    "{\\rm num}_2(\\beta)  = \\exp\\left( - {\\frac 1 \\xi_p} (\\kappa -1) \\left[ \\gamma_1 \\beta + \\gamma_2 \\beta^2 f + \\gamma_2^+ \\beta\\left(\\beta f - {\\overline \\gamma} \\right) \\mathbb{1}(\\beta f > {\\overline \\gamma} ) \\right] \\exp(r) {\\widehat e}\\right),\n",
    "\\end{eqnarray}\n",
    "and then compute\n",
    "\\begin{eqnarray}\n",
    "{\\rm scale}_2 = \\int_\\beta {\\rm num}_2(\\beta) p(\\beta) d\\beta\n",
    "\\end{eqnarray}\n",
    "\n",
    "via \"Gauss-Legendre\"  quadrature. We then form a density conditioned on model two:\n",
    "\\begin{eqnarray}\n",
    "{\\widetilde q}_2(\\beta)  = {\\frac {{\\rm num}(\\beta)}{{\\rm scale}2}}\n",
    "\\end{eqnarray}\n",
    "\n",
    "relative to $p(\\beta)$. We call ${\\widetilde q}_2(\\beta)$  the implied ambiguity-adjusted probabilities.\n",
    "\n",
    "Next, we compute\n",
    "\\begin{eqnarray}\n",
    "{\\mathcal  I}_2 =  -\\xi_p \\log {\\rm scale}_2,\n",
    "\\end{eqnarray}\n",
    "\n",
    "along with\n",
    "\n",
    "\\begin{eqnarray}\n",
    "{\\mathcal J}_2(e) = {\\frac {(\\kappa - 1) \\left( \\displaystyle\\int_\\beta    \\left[ \\gamma_1 \\beta + \\gamma_2 \\beta^2 f + \\gamma_2^+\\beta\\left(\\beta f - {\\overline \\gamma} \\right) \\mathbb{1}(\\beta f > {\\overline \\gamma} ) \\right] \n",
    "{\\rm num}_2(\\beta)p(\\beta) d \\beta \\right) \\exp(r) e}{{\\rm scale}_2}},\n",
    "\\end{eqnarray}\n",
    "\n",
    "using \"Gauss-Legendre\" quadratre for the numerator integral. \n",
    "In computing the integral term that involves high damage models, we choose \"Gauss-Legendre\" quadrature with 30 integration points on $[\\beta_f-5\\sigma_f,\\beta_f+5\\sigma_f] $. The __quad_int__ function replicates this calculation and can be found at './src/supportfunctions.py' .\n",
    "\n",
    "Notice that ${\\mathcal J}_2(e)$ can be computed for an arbitrary $e$ since the numerator integral does not depend on $e$.\n",
    "\n",
    "With these computations in hand, we form the relative entropy conditioned on model two:\n",
    "\\begin{eqnarray}\n",
    "{\\mathcal R}_2 = {\\frac 1 {\\xi_p}} \\left[{\\mathcal I}_2 - {\\mathcal J}_2\\left( {\\widehat e} \\right) \\right] .\n",
    "\\end{eqnarray}\n",
    "Notice that ${\\mathcal J}_2(e)$ can be computed for an arbitrary $e$ since the numerator integral does not depend on $e$.\n",
    "\n",
    "##### Distorted model probabilities\n",
    "\n",
    "Worst-case probabilities over models satisfy\n",
    "\n",
    "\\begin{eqnarray}\n",
    "{\\widetilde \\pi}_i \\  \\ \\propto  \\  \\ {\\widehat \\pi}_i \\exp\\left( -{\\frac 1 {\\xi_p}} {\\mathcal I}_i \\right),\n",
    "\\end{eqnarray}\n",
    "\n",
    "where the constant of proportionality scales the right-hand side so that the resulting ${\\tilde \\pi}_i$ sum to one.\n",
    "\n",
    "We start by forming\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\delta\\alpha(\\log e) - v_{r}e +v_{F}\\exp(r) e + \\left[ {\\widetilde \\pi}_1 {\\mathcal J}_1(e) + {\\widetilde \\pi}_2 {\\mathcal J}_2(e) \\right],\n",
    "\\end{eqnarray}\n",
    "\n",
    "and maximize with respect to $e$ with solution $e^*$.\n",
    "Also, we update the objective with a new entropy penalty give by\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\xi_p {\\mathcal R},\n",
    "\\end{eqnarray}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{eqnarray}\n",
    "{\\mathcal R} =  {\\widetilde \\pi}_1 {\\mathcal R}_1 + {\\widetilde \\pi}_2 {\\mathcal R}_2  + {\\widetilde \\pi}_1 \\left( \\log {\\widetilde \\pi}_1 - \\log {\\widehat \\pi}_1 \\right) + {\\widetilde \\pi}_2 \\left( \\log {\\widetilde \\pi}_2 - \\log {\\widehat \\pi}_2\\right),\n",
    "\\end{eqnarray}\n",
    "\n",
    "and with the optimized contribution\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\delta\\alpha(\\log e^*) - v_{r}e^* + \\left[ {\\widetilde \\pi}_1 {\\mathcal J}_1(e^*) + {\\widetilde \\pi}_2 {\\mathcal J}_2(e^*) \\right]\n",
    "\\end{eqnarray}\n",
    "\n",
    "along with the other contributions to the HJB equation.\n",
    "\n",
    "We then let ${\\widehat e} = e^*$ and repeat.\n",
    "\n",
    "##### Weighted damage models\n",
    "\n",
    "The weighted damage models seting calculates ${\\mathcal I}$, ${\\mathcal J}$ and ${\\mathcal R}$ using a weighted average of the low damage model and high damage model, where the weights come from thevdistorted model probabilities ${\\widetilde \\pi}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark:  Details for solving HJB PDE\n",
    "Recall that in order to solve the prefernce HJB model, we transform the question in solving a linear system of equations.\n",
    "\\begin{eqnarray}\n",
    "{\\frac {V(x) - {\\widetilde V}(x)} \\epsilon}  & =  &   V(x) \\cdot A(x)+{\\frac {\\partial V}{\\partial x}} (x) \\cdot  B(x)   + {\\frac {\\partial^2 V}{\\partial x \\partial x'}}(x) \\cdot C(x)+ D(x) \n",
    "\\end{eqnarray}\n",
    "Where\n",
    "\\begin{eqnarray}\n",
    "A(x) &=& - \\delta \\\\\n",
    "B(x) &=& {\\widehat \\mu}(x)\\\\\n",
    "C(x) &=& {\\frac 1 2} \\textrm{trace} \\left[\\sigma_X(x)' I \\sigma_X(x) \\right] \\\\\n",
    "D(x) &=& \\delta (1 - \\kappa) \\left( \\log \\left[ {\\alpha}  - {\\widehat i}(x)  - {\\widehat j} (x)  \\right] + k -  d  \\right) + \\delta \\kappa \\left[ \\log {\\widehat e}(x)   +  r \\right] + {\\frac {\\xi_m} 2} {\\widehat g}(x) \\cdot {\\widehat g}(x)   + \\xi_p {\\widehat {\\mathbb I}}(x) \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "Multiplying by $\\epsilon$ and rearranging the above equation gives:\n",
    "\n",
    "\\begin{align*} [\\epsilon  A(x) - 1] V(x) + \\epsilon {\\widehat B}(x) \\cdot {\\frac {\\partial V}{\\partial x}} (x) + \\epsilon \\hskip{.05in}\\rm{trace}\\left[\\frac {\\partial^2 V}{\\partial x \\partial x'}(x)C(x) \n",
    "\\right] + \\epsilon {\\widehat D}(x) + {\\widetilde V}(x) = 0\n",
    "\\end{align*}\n",
    "\n",
    "which yields the linear system of equation we aimed to solve in our c++ solver.\n",
    "\n",
    "Precisely speaking our value function is a three-dimension tensor $ V(R,T,K) $ with default grid size of (181,161,121) in \"F\" order. In order to solve the linear system, we reshape $ V(x) $ into a one dimension array with size of (3526061,1) and conduct similar adjustments to $ A(x) $, $ B(x) $, $ C(x) $ and $ D(x) $ respectively.\n",
    "\n",
    "In \"F\" order, when we call our initial value function at grid point (i,j,k) $ V(i,j,k) $, we instead call equivalent reshaped value fnction $V(i + j * N_i + k * N_i * N_j)$ where $N_i$, $N_j$ and $N_k$ stands for number of grid poitns in that dimension.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark: Conjugate gradient solver for linear system\n",
    "\n",
    "We solve the matrix counterpart to the equation in step (8) using the conjugate gradient algorithm.  This is a well known iterative algorithm designed to  solve a minimization problem:  $\\frac 1 2 (\\Lambda y - \\lambda)'(\\Lambda y - \\lambda)$ for a  nonsingular matrix $\\Lambda$ and vector $\\lambda$.   The $y$ that minimizes this expression  satisfies the linear equation $\\Lambda y = \\lambda$.  The matrix $\\Lambda$ and vector $\\lambda$ come from the numerical approximation of below equation.  We measure the conjugate gradient error by\n",
    "\n",
    "\\begin{equation}\n",
    "\\sqrt{{\\frac {(\\Lambda y - \\lambda)'(\\Lambda y - \\lambda)}{\\lambda'\\lambda}}}.\n",
    "\\end{equation}\n",
    "\n",
    "We prespecify a conjugate gradient error bound and a bound on the difference in value functions between iterations and take as the starting point for conjugate gradient the output from the previous iteration.  We achieve convergence when the difference in value functions between iterations satisfies a prespecified error bound.  Upon convergence, we compute the maximum error for the matrix approximation to the right-hand side of equation system in step (7). We call this the maximum pde error.\n",
    "\n",
    "#### Remark: PDE boundary conditions\n",
    "While we are computing one-sided difference approximations at boundary points, we are not imposing additional boundary conditions on our finite state space as is often done when solving pde's with regular boundaries. Instead we aim to approximate pde solutions for the stochastic differential equation with unattainable boundaries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark: Tolerance level and conergence criteria for HJB\n",
    "\n",
    "In choosing the tolerance level, we tested the time it takes to solve the PDE numerically is decreasing with the size of $\\epsilon$, while the stability of the program is also decreasing with $\\epsilon$. \n",
    "\\begin{eqnarray*}\n",
    "\\max_x |V(x)-\\widetilde{V}(x)| < tol\n",
    "\\tag{0}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "We choose time varying $\\epsilon$ to balance the efficiency of the program and for room to explore different magnitude of the uncertainty.\n",
    "* We used variable step sizes to speed program up while ensuring convergence: 0.3 for the first 1000 iterations, 0.2 for 1001-2000 iterations and 0.1 going forward.\n",
    "*  For averse low damage case, normal variable step sizes scheme didn't work so we used 0.05 after 7000 iterations. \n",
    "*  We used fixed step size at 0.1 for growth neutral case to ensure convergence\n",
    "\n",
    "The table belows lists the convergence criteria and final errors for solving the HJB equations for the different consumption damages settings. We solved all models listed in this table for the setting with damages to consumption by the false transient algorithm. Outer loop convergence was evaluted on the change in value functions. \n",
    "\n",
    "| ambiguity    |    damage    | Cobweb tolerance |  outer loop tolerance  | CG tolerance |  PDE Error | Iterations |\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|averse|  high  | 1e-5 | 1e-8 | 1e-10 | 6.88e-6 | 10394|\n",
    "|averse| weighted | 1e-5 | 1e-8 | 1e-10 | 6.25e-6| 10318|\n",
    "|averse|low| 1e-5 | 1e-8 | 1e-10 | 3.83e-6| 13596 |\n",
    "|neutral|high  | 1e-5 | 1e-8 | 1e-10 | 5.67e-6| 10424 |\n",
    "|neutral|weighted  | 1e-5 | 1e-8 | 1e-10 | 4.60e-6| 10427|\n",
    "|neutral|low  | 1e-5 | 1e-8 | 1e-10 | 3.73e-6| 20200 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark: Solving time and error analysis\n",
    "\n",
    "Below we outline the time to run our code for each tollerance level, referencing to the weighted damage models, ambiguity averse case. The green line denotes the cumulative running time while the red and blue lines denote error levels. Notice that the false transient error decreases log linearly across time while the PDE error its a lower bound that varies case by case.\n",
    "\n",
    "For the weighted damage models, ambiguity averse case, it took 10318 iterations to reach an outer loop error (defined by equation $(0)$) of 1e-8, a region where the PDE error essentially reaces its lower bound. It takes up to 18 hours to reach this tolerance level solving the HJB directly as decribed. The user can solve a coarse grid size version much faster. For example, it only takes 5 minutes to solve a similar problem of grid size 30 * 40 * 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T22:09:48.555549Z",
     "start_time": "2020-01-02T22:09:47.213855Z"
    }
   },
   "outputs": [],
   "source": [
    "error_log = pickle.load(open(\"./data/erroranalysis.pickle\", \"rb\", -1))\n",
    "x = np.arange(0,len(error_log['lhs_err'])+1)\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "fig.add_trace(go.Scatter(x = x, y = error_log['rhs_err'], name = \"PDE error (right)\",), secondary_y=True)\n",
    "fig.add_trace(go.Scatter(x = x, y = error_log['lhs_err'], name = \"False Transient error (right)\",), secondary_y=True)\n",
    "fig.add_trace(go.Scatter(x = x[2:], y = error_log['run_time'] / 3600, name = \"Running times\", ),secondary_y=False)\n",
    "fig.update_layout(title_text=\"errors and runtime by iterations\")\n",
    "\n",
    "# Set x-axis title\n",
    "fig.update_xaxes(title_text=\"Iterations\")\n",
    "\n",
    "# Set y-axes titles\n",
    "fig.update_layout(yaxis2 = dict(title=\"log10 scale Error\", type='log'), yaxis1 = dict(title=\"total time spent (hours)\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we simulated the trajectory given our model solutions. The initial values for the model solution simulations are given below:\n",
    "\n",
    "|Variable|Initial Value |\n",
    "|-|-|\n",
    "| $Y_0$       |  80  |\n",
    "| $K_0^{\\alpha}$ |666.67|\n",
    "| $R_0$ | 650 |\n",
    "| $F_0$ | 320 |\n",
    "| $D_0$ | $\\nabla \\Gamma(F_0)$ |\n",
    "\n",
    "The values for GDP comes from the World Bank database and the capital value is implied by the assumed productivity parameter and this GDP value. The values are in line with Hambel et al. (2018) and Pindyck and Wang (2013). The value for reserves comes from estimates of existing recoverable reserves of oil and coal from the EIA and BP (2018) and is consistent with the empirical measures of reserves cited by McGlade and Ekins (2015), who provide further details on this data, Golosov et al (2014), and Rogner (1997). \n",
    "The initial values of atmospheric temperature anomaly and atmospheric carbon concentration come from the NASA-GISS, NOAA, and Berkeley Earth datasets. The initial value of damages is implied by our damage function and the initial value of cumulative emissions.\n",
    "\n",
    "We remind readers here that evolution processes for log reserves (R), cumulative emssions(F) and log capitals (K) are:\n",
    "\n",
    "\\begin{equation}\n",
    "d\\log R_t = - \\left( {\\frac {E_t}{R_t} } \\right) dt + \\psi_0  \\left({\\frac {J_t}{R_t}} \\right)^{\\psi_1} dt -\n",
    "{\\frac {|\\sigma_R|^2} 2} dt  + \\sigma_R \\cdot dW_t,\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "d F_t =  E_t dt,\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "d \\log K_t = \\zeta_{K}(Z_{t})dt +  \\phi_0 \\log \\left(1 + \\phi_1 {\\frac {I_t}{K_t}} \\right) dt - {\\frac  {| \\sigma_K |^2} 2 dt}\n",
    "+\\sigma_{K}\\cdot dW_{t}.\n",
    "\\end{equation}\n",
    "\n",
    "We set all volatility parameters to 0.\n",
    "\n",
    "For numerical stability, we used linear interpolation in presenting plots in the paper. We provide code for both linear and cubic spline interpolation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T22:09:48.566521Z",
     "start_time": "2020-01-02T22:09:48.558541Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# Our customized interpolation class, which supports Cubic Spline and Linear interpolation for grid data\n",
    "class GridInterp():\n",
    "\n",
    "    def __init__(self, grids, values, method = 'Linear'):\n",
    "\n",
    "        # unpacking\n",
    "        self.grids = grids\n",
    "        (self.xs, self.ys, self.zs) = grids\n",
    "        self.nx = len(self.xs)\n",
    "        self.ny = len(self.ys)\n",
    "        self.nz = len(self.zs)\n",
    "        \n",
    "        self.values = values\n",
    "\n",
    "        assert (self.nx, self.ny, self.nz) == values.shape, \"ValueError: Dimensions not match\"\n",
    "        self.method = method\n",
    "\n",
    "    def get_value(self, x, y, z):\n",
    "\n",
    "        if self.method == 'Linear':\n",
    "            \n",
    "            func = RegularGridInterpolator(self.grids, self.values)\n",
    "            return func([x,y,z])[0]\n",
    "\n",
    "        elif self.method == 'Spline':\n",
    "\n",
    "            func1 = CubicSpline(self.xs, self.values)\n",
    "            yzSpace = func1(x)\n",
    "            \n",
    "            func2 = CubicSpline(self.ys, yzSpace)\n",
    "            zSpace = func2(y)\n",
    "            \n",
    "            func3 = CubicSpline(self.zs, zSpace)\n",
    "            return func3(z)\n",
    "\n",
    "        else:\n",
    "            raise ValueError('Method Not Supported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T22:10:23.908433Z",
     "start_time": "2020-01-02T22:09:48.568516Z"
    },
    "code_folding": [
     141
    ]
   },
   "outputs": [],
   "source": [
    "method = 'Linear' # Specify interpolating methods\n",
    "T = 100\n",
    "pers = 4 * T\n",
    "dt = T / pers\n",
    "nDims = 5\n",
    "its = 1\n",
    "\n",
    "gridpoints = (R, F, K)\n",
    "\n",
    "# emissions\n",
    "e_func_r = GridInterp(gridpoints, e, method)\n",
    "def e_func(x):\n",
    "    return e_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "# investment in reserves\n",
    "j_func_r = GridInterp(gridpoints, j, method)\n",
    "def j_func(x):\n",
    "    return max(j_func_r.get_value(np.log(x[0]), x[2], np.log(x[1])), 0)\n",
    "\n",
    "# investment in productive capitals\n",
    "i_func_r = GridInterp(gridpoints, i, method)\n",
    "def i_func(x):\n",
    "    return i_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "# first derivative of Value function w.r.t R\n",
    "v_drfunc_r = GridInterp(gridpoints, v0_dr, method)\n",
    "def v_drfunc(x):\n",
    "    return v_drfunc_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "# first derivative of Value function w.r.t F\n",
    "v_dffunc_r = GridInterp(gridpoints, v0_df, method)\n",
    "def v_dffunc(x):\n",
    "    return v_dffunc_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "# first derivative of Value function w.r.t K\n",
    "v_dkfunc_r = GridInterp(gridpoints, v0_dk, method)\n",
    "def v_dkfunc(x):\n",
    "    return v_dkfunc_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "# Value function\n",
    "v_func_r = GridInterp(gridpoints, v0, method)\n",
    "def v_func(x):\n",
    "    return v_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "# Distorted model probabilities\n",
    "pi_tilde_1_func_r = GridInterp(gridpoints, œÄÃÉ1 / (œÄÃÉ1 + œÄÃÉ2), method)\n",
    "def pi_tilde_1_func(x):\n",
    "    return pi_tilde_1_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "pi_tilde_2_func_r = GridInterp(gridpoints, œÄÃÉ2 / (œÄÃÉ1 + œÄÃÉ2), method)\n",
    "def pi_tilde_2_func(x):\n",
    "    return pi_tilde_2_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "# ambiguity adjusted probabilities\n",
    "def scale_2_fnc(x):\n",
    "    return np.exp(-1 / Œæp * xi_d * (Œ≥1 * x + Œ≥2 * x ** 2 * F_mat + Œ≥2_plus * x * (x * F_mat - FÃÑ) ** (power - 1) * ((x * F_mat - FÃÑ) >= 0)) * np.exp(R_mat) * e_hat)  * norm.pdf(x,Œ≤ùòß,np.sqrt(œÉ·µ¶))\n",
    "\n",
    "scale_2 = quad_int(scale_2_fnc, a, b, n, 'legendre')\n",
    "def q2_tilde_fnc(x):\n",
    "    return np.exp(-1 / Œæp * xi_d * (Œ≥1 * x + Œ≥2 * x ** 2 * F_mat + Œ≥2_plus * x * (x * F_mat - FÃÑ) ** (power - 1) * ((x * F_mat - FÃÑ) >= 0)) * np.exp(R_mat) * e_hat) / scale_2\n",
    "\n",
    "# Below are distorted model drifts for low damage and high damage models, see remark 3 for more details\n",
    "def base_model_drift_func(x):\n",
    "    return np.exp(R_mat) * e * (Œ≥1 * x + Œ≥2 * x ** 2 * F_mat + Œ≥ÃÑ2_plus * x * (x * F_mat - FÃÑ) ** (power - 1) * ((x * F_mat - FÃÑ) >= 0)) * norm.pdf(x,Œ≤ùòß,np.sqrt(œÉ·µ¶))\n",
    "base_model_drift =  quad_int(base_model_drift_func, a, b, n, 'legendre')\n",
    "\n",
    "mean_nordhaus = Œ≤ÃÉ1\n",
    "lambda_tilde_nordhaus = ŒªÃÉ1\n",
    "nordhaus_model_drift = (Œ≥1 * mean_nordhaus + Œ≥2 * (1 / lambda_tilde_nordhaus + mean_nordhaus ** 2) * F_mat) * np.exp(R_mat) * e\n",
    "\n",
    "def weitzman_model_drift_func(x):\n",
    "    return np.exp(R_mat) * e * q2_tilde_fnc(x) * (Œ≥1 * x + Œ≥2 * x ** 2 * F_mat + Œ≥2_plus * x * (x * F_mat - FÃÑ ) ** (power - 1) * ((x * F_mat - FÃÑ) >= 0)) * norm.pdf(x,Œ≤ùòß,np.sqrt(œÉ·µ¶))\n",
    "weitzman_model_drift = quad_int(weitzman_model_drift_func, a, b, n, 'legendre')\n",
    "\n",
    "nordhaus_drift_func_r = GridInterp(gridpoints, nordhaus_model_drift, method)\n",
    "def nordhaus_drift_func(x):\n",
    "    return nordhaus_drift_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "weitzman_drift_func_r = GridInterp(gridpoints, weitzman_model_drift, method)\n",
    "def weitzman_drift_func(x):\n",
    "    return weitzman_drift_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "base_drift_func_r = GridInterp(gridpoints, base_model_drift, method)\n",
    "def base_drift_func (x): \n",
    "    return base_drift_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "# drifts for each diffusion process\n",
    "def muR(x):\n",
    "    return -e_func(x) + œà0 * (j_func(x) * x[1] / x[0]) ** œà1\n",
    "def muK(x): \n",
    "    return (Œºk + œï0 * np.log(1 + i_func(x) * œï1))\n",
    "def muF(x):\n",
    "    return e_func(x) * x[0]\n",
    "def muD_base(x):\n",
    "    return base_drift_func(x)\n",
    "def muD_tilted(x):\n",
    "    return pi_tilde_1_func(x) * nordhaus_drift_func(x) + (1 - pi_tilde_1_func(x)) * weitzman_drift_func(x)\n",
    "\n",
    "# volatilities for each process\n",
    "def sigmaR(x):\n",
    "    return np.zeros(x[:5].shape)\n",
    "def sigmaK(x):\n",
    "    return np.zeros(x[:5].shape)\n",
    "def sigmaF(x):\n",
    "    return np.zeros(x[:5].shape)\n",
    "def sigmaD(x):\n",
    "    return np.zeros(x[:5].shape)\n",
    "\n",
    "# initial points\n",
    "R_0 = 650\n",
    "K_0 = 80 / Œ±\n",
    "F_0 = 870 - 580\n",
    "initial_val = np.array([R_0, K_0, F_0])\n",
    "D_0_base = muD_base(initial_val)\n",
    "D_0_tilted = muD_tilted(initial_val)\n",
    "\n",
    "# Set bounds\n",
    "R_max_sim = np.exp(max(R))\n",
    "K_max_sim = np.exp(max(K))\n",
    "F_max_sim = max(F)\n",
    "D_max_sim = 5.0\n",
    "\n",
    "R_min_sim = np.exp(min(R))\n",
    "K_min_sim = np.exp(min(K))\n",
    "F_min_sim = min(F)\n",
    "D_min_sim = -5\n",
    "\n",
    "upperbounds = np.array([R_max_sim, K_max_sim, F_max_sim, D_max_sim, D_max_sim])\n",
    "lowerbounds = np.array([R_min_sim, K_min_sim, F_min_sim, D_min_sim, D_min_sim])\n",
    "\n",
    "hists = np.zeros([pers, nDims, its])\n",
    "e_hists = np.zeros([pers,its])\n",
    "j_hists = np.zeros([pers,its])\n",
    "i_hists = np.zeros([pers,its])\n",
    "\n",
    "v_dr_hists = np.zeros([pers,its])\n",
    "v_df_hists = np.zeros([pers,its])\n",
    "v_dk_hists = np.zeros([pers,its])\n",
    "v_hists = np.zeros([pers,its])\n",
    "\n",
    "# Simulations\n",
    "for iters in range(0,its): # simulation path, currently it's only one path\n",
    "    hist = np.zeros([pers,nDims])\n",
    "    e_hist = np.zeros([pers,1])\n",
    "    i_hist = np.zeros([pers,1])\n",
    "    j_hist = np.zeros([pers,1])\n",
    "\n",
    "    v_dr_hist = np.zeros([pers,1])\n",
    "    v_df_hist = np.zeros([pers,1])\n",
    "    v_dk_hist = np.zeros([pers,1])\n",
    "    v_hist = np.zeros([pers,1])\n",
    "\n",
    "    hist[0,:] = [R_0, K_0, F_0, D_0_base, D_0_tilted]\n",
    "    e_hist[0] = e_func(hist[0,:]) * hist[0,0]\n",
    "    i_hist[0] = i_func(hist[0,:]) * hist[0,1]\n",
    "    j_hist[0] = j_func(hist[0,:]) * hist[0,0]\n",
    "    v_dr_hist[0] = v_drfunc(hist[0,:])\n",
    "    v_df_hist[0] = v_dffunc(hist[0,:])\n",
    "    v_dk_hist[0] = v_dkfunc(hist[0,:])\n",
    "    v_hist[0] = v_func(hist[0,:])\n",
    "\n",
    "    for tm in range(1,pers): # time \n",
    "        shock = norm.rvs(0,np.sqrt(dt),nDims)\n",
    "        # print(muR(hist[tm-1,:]))\n",
    "        hist[tm,0] = cap(hist[tm-1,0] * np.exp((muR(hist[tm-1,:])- 0.5 * sum((sigmaR(hist[tm-1,:])) ** 2))* dt + sigmaR(hist[tm-1,:]).dot(shock)),lowerbounds[0], upperbounds[0])\n",
    "        hist[tm,1] = cap(hist[tm-1,1] * np.exp((muK(hist[tm-1,:])- 0.5 * sum((sigmaK(hist[tm-1,:])) ** 2))* dt + sigmaK(hist[tm-1,:]).dot(shock)),lowerbounds[1], upperbounds[1])\n",
    "        hist[tm,2] = cap(hist[tm-1,2] + muF(hist[tm-1,:]) * dt + sigmaF(hist[tm-1,:]).dot(shock), lowerbounds[2], upperbounds[2])\n",
    "        hist[tm,3] = cap(hist[tm-1,3] + muD_base(hist[tm-1,:]) * dt + sigmaD(hist[tm-1,:]).dot(shock), lowerbounds[3], upperbounds[3])\n",
    "        hist[tm,4] = cap(hist[tm-1,4] + muD_tilted(hist[tm-1,:]) * dt + sigmaD(hist[tm-1,:]).dot(shock), lowerbounds[4], upperbounds[4])\n",
    "\n",
    "        e_hist[tm] = e_func(hist[tm-1,:]) * hist[tm-1,0]\n",
    "        i_hist[tm] = i_func(hist[tm-1,:]) * hist[tm-1,1]\n",
    "        j_hist[tm] = j_func(hist[tm-1,:]) * hist[tm-1,0]\n",
    "\n",
    "        v_dr_hist[tm] = v_drfunc(hist[tm-1,:])\n",
    "        v_df_hist[tm] = v_dffunc(hist[tm-1,:])\n",
    "        v_dk_hist[tm] = v_dkfunc(hist[tm-1,:])\n",
    "        v_hist[tm] = v_func(hist[tm-1,:])\n",
    "\n",
    "    hists[:,:,iters] = hist\n",
    "    e_hists[:,[iters]] = e_hist\n",
    "    i_hists[:,[iters]] = i_hist\n",
    "    j_hists[:,[iters]] = j_hist\n",
    "\n",
    "    v_dr_hists[:,[iters]] = v_dr_hist\n",
    "    v_df_hists[:,[iters]] = v_df_hist\n",
    "    v_dk_hists[:,[iters]] = v_dk_hist\n",
    "    v_hists[:,[iters]] = v_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCC Calculation Feyman Kac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part details the steps to calculate and decompose social cost of carbon. More details about economic meanings and derivation coule be found in Section 3.1.2, Section 4.7 and online appendix B.\n",
    "\n",
    "The term social cost of carbon (SCC) refers to the social marginal rate of substitution between emissions and consumption.  It is a shadow price of the resource allocation problem for a hypothetical planner and given by equation $(1)$.\n",
    "\n",
    "\\begin{equation}\n",
    "{scc} (x) = \\left[{\\frac {V_r(x )}{\\exp(r) }} - V_f(x)\n",
    "+  (1 - \\kappa )  \\left([\\nabla \\Gamma](\\beta f) \\beta  + \\zeta_D(z) \\cdot \\begin{bmatrix} 1 \\cr 0 \\end{bmatrix} \\right)\\right]\n",
    "\\left[{\\frac {\\alpha  - i^*(x)  - j^*(x) } {\\delta (1 - \\kappa)}}\\right]\n",
    "\\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "We decomposes social cost cost carbon into three parts: social cost induced by non-externality, social cost induced by externality and social cost induced by uncertainty. \n",
    "\n",
    "The first term in formula $(1)$ accounts for __social cost induced by non-externality__ which we called \"private component\" in the code. It has nothing to do with damages to the climate in our model and represents resource scarcity.\n",
    "\n",
    "The __social cost induced by externality__ was captured by:\n",
    "\n",
    "\\begin{equation} \\label{external_cost}\n",
    "ecc(x)  = - V_f(x)   +  (1-\\kappa)   \\left([\\nabla \\Gamma](\\beta f) \\beta  + \\zeta_D(z) \\cdot \\begin{bmatrix} 1 \\cr 0 \\end{bmatrix} \\right),\n",
    "\\end{equation}\n",
    "\n",
    "scaled by current period marginal utility of consumption,\n",
    "or equivalently as we showed in online appendix B,\n",
    "\n",
    "\\begin{align} \\label{discounted_cost}\n",
    "ecc(x) =\n",
    " (1-\\kappa) & {\\mathbb E} \\left[ \\int_0^\\infty \\exp(-\\delta \\tau)\n",
    "  \\left[\\nabla^2 \\Gamma\\right]( \\beta F_{t+\\tau} ) \\beta^2 E_{t+\\tau} d \\tau \\mid X_t = x \\right] + \\left[\\nabla \\Gamma \\right](\\beta F_t) \\beta  + \\zeta_D(Z_t) \\cdot \\begin{bmatrix} 1 \\cr 0 \\end{bmatrix}\n",
    "\\tag{2}\n",
    "\\end{align}\n",
    "\n",
    "divided by the current period marinal utility of consumption.\n",
    "\n",
    "An more interesting component for us to investigate would be __social cost induced by uncerntainty__. As an alternative to evaluating the discounted value using the ambiguity-adjusted probability ${\\widetilde q}_2(\\beta)$, suppose we use the original unadjusted probabilities to evaluate the expected discounted value of the future marginal social costs.  Call this $\\overline{ecc}(x)$. We take the difference between the two:\n",
    "\n",
    "\\begin{equation}\n",
    "ucc^*(x) = \\left[ ecc^*(x)  - \\overline{ecc}(x)  \\right]\n",
    "\\end{equation}\n",
    "\\begin{eqnarray}\n",
    "ecc^*(x) = (1-\\kappa) \\int_{\\Theta} \\left( [\\nabla \\Gamma ](\\beta f) \\beta + \\zeta_D(z) \\cdot \\begin{bmatrix} 1 \\cr 0 \\end{bmatrix} \\right) q^*(\\theta | x) P(d\\theta) + exp(\\delta t)  \\int_0^\\infty \\exp(- \\delta t) \\mathbb{E} \\left[ \\Psi^*(X_{\\tau}) \\mid X_t \\right] d \\tau\n",
    "\\tag{3}\\\\\n",
    "\\overline{ecc}(x) = (1-\\kappa) \\int_{\\Theta} \\left( [\\nabla \\Gamma ](\\beta f) \\beta + \\zeta_D(z) \\cdot \\begin{bmatrix} 1 \\cr 0 \\end{bmatrix} \\right) P(d\\theta) + \\exp(\\delta t)  \\int_0^\\infty \\exp(- \\delta t) \\mathbb{E} \\left[ \\bar{\\Psi}(X_{\\tau}) \\mid X_t \\right] d \\tau \\tag{4}\\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "where$\\Psi^*(x)$ and $\\bar{\\Psi}(x)$ are given by\n",
    "\\begin{align}\n",
    "\\Psi^*(x) & = (1-\\kappa) \\int_{\\Theta} [ \\nabla^2  \\Gamma ] ( \\beta f ) \\beta^2  e^*(x) \\exp(r) q^*(\\theta|x)P(d\\theta) \\tag{5}\\\\\n",
    "\\bar{\\Psi}(x) & = (1-\\kappa) \\int_{\\Theta} [ \\nabla^2  \\Gamma ] ( \\beta f ) \\beta^2  e^*(x) \\exp(r) P(d\\theta) \\tag{6}\n",
    "\\end{align}\n",
    "\n",
    "$[\\nabla^2 \\Gamma]$ denotes the second derivative of $\\Gamma$, and $e^*$ denotes the socially choice of emissions.\n",
    "\n",
    "\n",
    "Applying the Feyman-Kac (F-K) formula to these expressions gives alternative PDE characterizations of $\\Phi^*(X)$ and $\\bar{\\Phi}(X)$:\n",
    "\n",
    "\\begin{align*}\n",
    "-\\delta \\Phi^*(x) + {\\frac {\\partial \\Phi^*}{\\partial x}} (x) \\cdot \\mu_X[x,a^*(x)] +{\\frac 1 2} \\textrm{trace} \\left[\\sigma_X(x)' {\\frac {\\partial^2 \\Phi^*}{\\partial x \\partial x'}}(x) \\sigma_X(x) \\right] + \\Psi^*(x) & = 0 \\tag{7} \\\\\n",
    "-\\delta \\bar{\\Phi}(x) + {\\frac {\\partial \\bar{\\Phi}}{\\partial x}} (x) \\cdot \\mu_X[x,a^*(x)] +{\\frac 1 2} \\textrm{trace} \\left[\\sigma_X(x)' {\\frac {\\partial^2 \\bar{\\Phi}}{\\partial x \\partial x'}}(x) \\sigma_X(x) \\right] + \\bar{\\Psi}(x) & = 0. \\tag{8}\n",
    "\\end{align*}\n",
    "\n",
    "We again solve the PDE using the conjugate gradient solver using the above form.\n",
    "\n",
    "As was the case with the planner's value function, we need to solve the PDEs that result from the F-K formula numerically. We approximate the derivatives of $\\Phi^*(x)$ and $\\bar{\\Phi}(x)$ with the same finite difference schemes as outlined in the Numerical Methods for the HJB Equation section. Solving for $\\Phi^*(x)$ and $\\bar{\\Phi}(x)$ after plugging in the finite difference approximations of the derivatives to the F-K expressions results in unconditionally linear equations for $\\Phi^*(x)$ and $\\bar{\\Phi}(x)$. We can therefore solve for $\\Phi^*(x)$ and $\\bar{\\Phi}(x)$ directly from these linear equations by approximately inverting the linear systems using the conjugate gradient method. This is done without the use of the false transient or other iterative schemes as we do not face the same nonlinearities and instabilities that arose in the HJB equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T22:11:08.489646Z",
     "start_time": "2020-01-02T22:10:23.910357Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "if Œæp > 100:  # We consider for Œæp level over 100 as \n",
    "    \n",
    "    # Under ambiguity neutrality, uncertainty wouldn't play a role in our decomposition.\n",
    "    # Below three lines replicates equation (1)\n",
    "    MC = Œ¥ * (1-Œ∫) / (Œ± * np.exp(K_mat) - i * np.exp(K_mat) - j * np.exp(R_mat))      # Marginal utility of consumption\n",
    "    ME = Œ¥ * Œ∫ / (e * np.exp(R_mat))      # Marginal utility of emission\n",
    "    SCC = 1000 * ME / MC\n",
    "    SCC_func_r = GridInterp(gridpoints, SCC, method)\n",
    "    \n",
    "    def SCC_func(x): \n",
    "        return SCC_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "    \n",
    "    SCC_values = np.zeros([pers,its])\n",
    "    \n",
    "    # Interpolating values from simulated trajectory\n",
    "    for tm in range(pers):\n",
    "        for path in range(its): \n",
    "            SCC_values[tm, path] = SCC_func(hists[tm,:,path])\n",
    "\n",
    "    SCC_total = np.mean(SCC_values,axis = 1)\n",
    "\n",
    "    SCCs['SCC'] = SCC_total\n",
    "    \n",
    "else:\n",
    "\n",
    "    # Solving Feyman Kac PDE for equation (8)\n",
    "    \n",
    "    # below three lines replicates euqation (6)\n",
    "    def base_model_flow_func(x):\n",
    "        return (Œ≥2 * x ** 2 + Œ≥ÃÑ2_plus * x ** 2 * ((x * F_mat - FÃÑ) >=0)) * np.exp(R_mat) * e *  norm.pdf(x,Œ≤ùòß,np.sqrt(œÉ·µ¶))\n",
    "    base_model_flow = quad_int(base_model_flow_func, a, b, n, 'legendre')\n",
    "    flow_base = base_model_flow\n",
    "\n",
    "    # input for solver\n",
    "\n",
    "    A = -Œ¥ * np.ones(R_mat.shape)\n",
    "    B_r = -e + œà0 * (j ** œà1) * np.exp(œà1 * (K_mat - R_mat)) - 0.5 * (œÉùò≥ ** 2)\n",
    "    B_k = Œºk + œï0 * np.log(1 + i * œï1) - 0.5 * (œÉùò¨ ** 2)\n",
    "    B_f = e * np.exp(R_mat)\n",
    "    C_rr = 0.5 * œÉùò≥ ** 2 * np.ones(R_mat.shape)\n",
    "    C_kk = 0.5 * œÉùò¨ ** 2 * np.ones(R_mat.shape)\n",
    "    C_ff = np.zeros(R_mat.shape)\n",
    "    D = flow_base\n",
    "    \n",
    "    if smart_guess:\n",
    "        out = PDESolver(stateSpace, A, B_r, B_f, B_k, C_rr, C_ff, C_kk, D, base_guess, Œµ, smartguess = True, solverType='Feyman Kac')\n",
    "\n",
    "    else:\n",
    "        base_guess = np.zeros(R_mat.shape)\n",
    "        out = PDESolver(stateSpace, A, B_r, B_f, B_k, C_rr, C_ff, C_kk, D, base_guess, Œµ, smartguess = False, solverType='Feyman Kac')\n",
    "\n",
    "    v0_base = out[2].reshape(v0.shape, order=\"F\")\n",
    "    v0_base = v0_base\n",
    "\n",
    "    # Calculating PDE Error\n",
    "    v0_dr_base = finiteDiff(v0_base,0,1,hR) \n",
    "    v0_df_base = finiteDiff(v0_base,1,1,hF)\n",
    "    v0_dk_base = finiteDiff(v0_base,2,1,hK)\n",
    "\n",
    "    v0_drr_base = finiteDiff(v0_base,0,2,hR)\n",
    "    v0_dff_base = finiteDiff(v0_base,1,2,hF)\n",
    "    v0_dkk_base = finiteDiff(v0_base,2,2,hK)\n",
    "\n",
    "    v0_drr_base[v0_dr_base < 1e-16] = 0\n",
    "    v0_dr_base[v0_dr_base < 1e-16] = 1e-16\n",
    "\n",
    "    PDE_rhs = A * v0_base + B_r * v0_dr_base + B_f * v0_df_base + B_k * v0_dk_base + C_rr * v0_drr_base + C_kk * v0_dkk_base + C_ff * v0_dff_base + D\n",
    "    PDE_Err = np.max(abs(PDE_rhs))\n",
    "    print(\"Feyman Kac Base Model Solved. PDE Error: {:.10f}; Iterations: {:d}; CG Error: {:.10f}\".format(PDE_Err, out[0], out[1]))\n",
    "\n",
    "    # Solving Feyman Kac PDE for equation (7)\n",
    "    \n",
    "    # Generates the ambiguity averse adjusted probability functions\n",
    "    mean_nordhaus = Œ≤ÃÉ1\n",
    "    lambda_tilde_nordhaus = ŒªÃÉ1\n",
    "    def scale_2_fnc(x):\n",
    "        return np.exp(-1 / Œæp * xi_d * (Œ≥1 * x + Œ≥2 * x ** 2 * F_mat + Œ≥2_plus * x * (x * F_mat - FÃÑ) ** (power - 1) * ((x * F_mat - FÃÑ) >= 0)) * np.exp(R_mat) * e)  * norm.pdf(x,Œ≤ùòß,np.sqrt(œÉ·µ¶))\n",
    "\n",
    "    scale_2 = quad_int(scale_2_fnc, a, b, n, 'legendre')\n",
    "\n",
    "    def q2_tilde_fnc(x):\n",
    "        return np.exp(-1 / Œæp * xi_d * (Œ≥1 * x + Œ≥2 * x ** 2 * F_mat + Œ≥2_plus * x * (x * F_mat - FÃÑ) ** (power - 1) * ((x * F_mat - FÃÑ) >= 0)) * np.exp(R_mat) * e) / scale_2\n",
    "\n",
    "    ## Construction of equation (5)\n",
    "    # See Remark 3 for more details\n",
    "    nordhaus_model_flow = (Œ≥2 * (1 / lambda_tilde_nordhaus + mean_nordhaus ** 2)) * np.exp(R_mat) * e \n",
    "    def weitzman_model_flow_func(x): \n",
    "        return q2_tilde_fnc(x) * (Œ≥2 * x ** 2 + Œ≥2_plus * x ** 2 * ((x * F_mat - FÃÑ) >= 0 )) * np.exp(R_mat) * e * norm.pdf(x,Œ≤ùòß,np.sqrt(œÉ·µ¶))\n",
    "    weitzman_model_flow = quad_int(weitzman_model_flow_func, a, b, n, 'legendre')\n",
    "\n",
    "    I1 = a1 - 0.5 * np.log(Œª) * Œæp + 0.5 * np.log(ŒªÃÉ1) * Œæp + 0.5 * Œª * Œ≤ùòß ** 2 * Œæp - 0.5 * ŒªÃÉ1 * (Œ≤ÃÉ1) ** 2 * Œæp\n",
    "    I2 = -1 * Œæp * np.log(scale_2)\n",
    "    œÄÃÉ1 = (weight) * np.exp(-1 / Œæp * I1)\n",
    "    œÄÃÉ2 = (1 - weight) * np.exp(-1 / Œæp * I2)\n",
    "    œÄÃÉ1_norm = œÄÃÉ1 / (œÄÃÉ1 + œÄÃÉ2)\n",
    "    œÄÃÉ2_norm = 1 - œÄÃÉ1_norm\n",
    "\n",
    "    flow_tilted = œÄÃÉ1_norm * nordhaus_model_flow + œÄÃÉ2_norm * weitzman_model_flow\n",
    "\n",
    "    # Formulating Feyman Kac PDE\n",
    "    A = -Œ¥ * np.ones(R_mat.shape)\n",
    "    B_r = -e + œà0 * (j ** œà1) * np.exp(œà1 * (K_mat - R_mat)) - 0.5 * (œÉùò≥ ** 2)\n",
    "    B_k = Œºk + œï0 * np.log(1 + i * œï1) - 0.5 * (œÉùò¨ ** 2)\n",
    "    B_f = e * np.exp(R_mat)\n",
    "    C_rr = 0.5 * œÉùò≥ ** 2 * np.ones(R_mat.shape)\n",
    "    C_kk = 0.5 * œÉùò¨ ** 2 * np.ones(R_mat.shape)\n",
    "    C_ff = np.zeros(R_mat.shape)\n",
    "    D = flow_tilted\n",
    "    \n",
    "    if smart_guess:\n",
    "        out = PDESolver(stateSpace, A, B_r, B_f, B_k, C_rr, C_ff, C_kk, D, worst_guess, Œµ, smartguess = True, solverType='Feyman Kac')\n",
    "\n",
    "    else:\n",
    "        worst_guess = np.zeros(R_mat.shape)\n",
    "        out = PDESolver(stateSpace, A, B_r, B_f, B_k, C_rr, C_ff, C_kk, D, worst_guess, Œµ, smartguess = False, solverType='Feyman Kac')\n",
    "    v0_worst = out[2].reshape(v0.shape, order=\"F\")\n",
    "    v0_worst = v0_worst\n",
    "    \n",
    "    # Calculating PDE Error for solving worst case Feyman Kac\n",
    "    v0_dr_worst = finiteDiff(v0_worst,0,1,hR) \n",
    "    v0_df_worst = finiteDiff(v0_worst,1,1,hF)\n",
    "    v0_dk_worst = finiteDiff(v0_worst,2,1,hK)\n",
    "\n",
    "    v0_drr_worst = finiteDiff(v0_worst,0,2,hR)\n",
    "    v0_dff_worst = finiteDiff(v0_worst,1,2,hF)\n",
    "    v0_dkk_worst = finiteDiff(v0_worst,2,2,hK)\n",
    "    v0_drr_worst[v0_dr_worst < 1e-16] = 0\n",
    "    v0_dr_worst[v0_dr_worst < 1e-16] = 1e-16\n",
    "\n",
    "    PDE_rhs = A * v0_worst + B_r * v0_dr_worst + B_f * v0_df_worst + B_k * v0_dk_worst + C_rr * v0_drr_worst + C_kk * v0_dkk_worst + C_ff * v0_dff_worst + D\n",
    "    PDE_Err = np.max(abs(PDE_rhs))\n",
    "    print(\"Feyman Kac Worst Model Solved. PDE Error: {:.10f}; Iterations: {:d}; CG Error: {:.10f}\".format(PDE_Err, out[0], out[1]))\n",
    "\n",
    "\n",
    "    # SCC decomposition\n",
    "\n",
    "    v0_dr = finiteDiff(v0,0,1,hR) \n",
    "    v0_df = finiteDiff(v0,1,1,hF)\n",
    "    v0_dk = finiteDiff(v0,2,1,hK)\n",
    "    v0_dr[v0_dr < 1e-16] = 1e-16\n",
    "\n",
    "    gridpoints = (R, F, K)  # can modify\n",
    "\n",
    "    # Total SCC, equation (1)\n",
    "    MC = Œ¥ * (1-Œ∫) / (Œ± * np.exp(K_mat) - i * np.exp(K_mat) - j * np.exp(R_mat))\n",
    "    ME = Œ¥ * Œ∫ / (e * np.exp(R_mat))\n",
    "    SCC = 1000 * ME / MC\n",
    "    SCC_func_r = GridInterp(gridpoints, SCC, method)\n",
    "\n",
    "    def SCC_func(x): \n",
    "        return SCC_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "    # private SCC, first part of equation (1)\n",
    "    ME1 = v0_dr * np.exp(-R_mat)\n",
    "    SCC1 = 1000 * ME1 / MC\n",
    "    SCC1_func_r = GridInterp(gridpoints, SCC1, method)\n",
    "    def SCC1_func(x):\n",
    "        return SCC1_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "    # Feyman Kac solution pluged in equaion (6) - second part of equation (4)\n",
    "    ME2_base = (1-Œ∫) * v0_base\n",
    "    SCC2_base = 1000 * ME2_base / MC\n",
    "    SCC2_base_func_r = GridInterp(gridpoints, SCC2_base, method)\n",
    "    def SCC2_base_func(x):\n",
    "        return SCC2_base_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "    # first part of equation (4)\n",
    "    def V_d_baseline_func(x):\n",
    "        return xi_d * (Œ≥1 * x + Œ≥2 * F_mat * x** 2 +\n",
    "                        Œ≥ÃÑ2_plus * x * (x * F_mat - FÃÑ) * (power - 1)\n",
    "                        * ((x * F_mat - FÃÑ) >= 0 )) * norm.pdf(x, Œ≤ùòß, np.sqrt(œÉ·µ¶))\n",
    "    V_d_baseline = quad_int(V_d_baseline_func, a, b, n, 'legendre')\n",
    "    ME2b = -V_d_baseline\n",
    "    SCC2_V_d_baseline = 1000 * ME2b / MC\n",
    "    SCC2_V_d_baseline_func_r = GridInterp(gridpoints, SCC2_V_d_baseline, method)\n",
    "    def SCC2_V_d_baseline_func(x):\n",
    "        return SCC2_V_d_baseline_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "    # Feyman Kac solution pluged in equaion (5) - second part of equation (3)\n",
    "    ME2_tilt = (1-Œ∫) * v0_worst\n",
    "    SCC2_tilt = 1000 * ME2_tilt / MC\n",
    "    SCC2_tilt_func_r = GridInterp(gridpoints, SCC2_tilt, method)\n",
    "    def SCC2_tilt_func(x):\n",
    "        return SCC2_tilt_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "    # first part of equation (3)\n",
    "    ME2b = -expec_e_sum * np.exp(-R_mat)\n",
    "    SCC2_V_d_tilt_ = 1000 * ME2b / MC\n",
    "    SCC2_V_d_tilt_func_r = GridInterp(gridpoints, SCC2_V_d_tilt_, method)\n",
    "    def SCC2_V_d_tilt_func(x):\n",
    "        return SCC2_V_d_tilt_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "    # Interpolating via simulated trajectories \n",
    "    SCC_values = np.zeros([pers,its])\n",
    "    SCC1_values = np.zeros([pers,its])\n",
    "    SCC2_base_values = np.zeros([pers,its])\n",
    "    SCC2_tilt_values = np.zeros([pers,its])\n",
    "    SCC2_V_d_baseline_values = np.zeros([pers,its])\n",
    "    SCC2_V_d_tilt_values = np.zeros([pers,its])\n",
    "\n",
    "    for tm in range(pers): # time horizons\n",
    "        for path in range(its): # number of simulation path, currently it's only one path\n",
    "            SCC_values[tm, path] = SCC_func(hists[tm,:,path])\n",
    "            SCC1_values[tm, path] = SCC1_func(hists[tm,:,path])\n",
    "            SCC2_base_values[tm, path] = SCC2_base_func(hists[tm,:,path]) \n",
    "            SCC2_tilt_values[tm, path] = SCC2_tilt_func(hists[tm,:,path])\n",
    "            SCC2_V_d_baseline_values[tm, path] = SCC2_V_d_baseline_func(hists[tm,:,path])\n",
    "            SCC2_V_d_tilt_values[tm, path] = SCC2_V_d_tilt_func(hists[tm,:,path])\n",
    "\n",
    "    SCC_total = np.mean(SCC_values,axis = 1)     # Total SCC, \n",
    "    SCC_private = np.mean(SCC1_values,axis = 1)  # Private SCC, non externality\n",
    "    SCC2_FK_base = np.mean(SCC2_base_values,axis = 1)  # ecc_bar, expectation of future horizons, second part of equation (4)\n",
    "    SCC2_FK_tilt = np.mean(SCC2_tilt_values,axis = 1)  # ecc_star, expectation of future horizons, second part of equation (3)\n",
    "    SCC2_V_d_baseline = np.mean(SCC2_V_d_baseline_values,axis = 1) # ecc_bar first part; instantaneous contribution, second part of equation (4)\n",
    "    SCC2_V_d_tilt = np.mean(SCC2_V_d_tilt_values,axis = 1)  # ecc_star first part; instantaneous contribution, second part of equation (3)\n",
    "\n",
    "    SCCs = {}\n",
    "    SCCs['SCC'] = SCC_total # total\n",
    "    SCCs['SCC1'] = SCC_private # private\n",
    "    SCCs['SCC2'] = SCC2_FK_base + SCC2_V_d_baseline   # external\n",
    "    SCCs['SCC3'] = SCC2_V_d_tilt - SCC2_V_d_baseline + SCC2_FK_tilt - SCC2_FK_base  # uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark: Convergence criteria for Feyman Kac\n",
    "Below table listed the convergence criteria and final errors for solving HJBs at different preference setting which we used in the paper. As the statespace was over 3 millions, it's even slow for Conjugate Gradient to converge to a small error. Here we cap the max iteration in conjugate gradient to 400,000.\n",
    "\n",
    "| ambiguity    |   damage specification   | iterations | CG Error |  PDE Error |\n",
    "|:-:|:-:|:-:|:-:|:-:|\n",
    "|averse | weighted, base  | 400,000 | 3e-6 | 3.29e-8|\n",
    "|averse | weighted, worst | 400,000 | 4e-6 | 6.94e-8|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we caculated and plotted the ${\\widetilde q}_2(\\beta)$  implied Ambiguity-Adjusted Probabilities for each model, please see remark 3 for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T22:11:08.879354Z",
     "start_time": "2020-01-02T22:11:08.491578Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "REs = {}\n",
    "Dists = {}\n",
    "a = Œ≤ùòß - 5 * np.sqrt(œÉ·µ¶)\n",
    "b = Œ≤ùòß + 5 * np.sqrt(œÉ·µ¶)\n",
    "a_10std = Œ≤ùòß - 10 * np.sqrt(œÉ·µ¶)\n",
    "b_10std = Œ≤ùòß + 10 * np.sqrt(œÉ·µ¶)\n",
    "\n",
    "RE_func_r = GridInterp(gridpoints, RE, method)\n",
    "def RE_func(x):\n",
    "    return RE_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "e_func_r = GridInterp(gridpoints, e, method)\n",
    "def e_func(x):\n",
    "    return e_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "pi_tilde_1_func_r = GridInterp(gridpoints, œÄÃÉ1, method)\n",
    "def pi_tilde_1_func(x):\n",
    "    return pi_tilde_1_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "lambda_tilde_1_func_r = GridInterp(gridpoints, ŒªÃÉ1, method)\n",
    "def lambda_tilde_1_func(x):\n",
    "    return lambda_tilde_1_func_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "beta_tilde_1_r = GridInterp(gridpoints, Œ≤ÃÉ1, method)\n",
    "def beta_tilde_1_func(x):\n",
    "    return beta_tilde_1_r.get_value(np.log(x[0]), x[2], np.log(x[1]))\n",
    "\n",
    "RE_plot = np.zeros(pers)\n",
    "weight_plot = np.zeros(pers)\n",
    "beta_f_space = np.linspace(a_10std,b_10std,200)\n",
    "\n",
    "#Relative Entropy\n",
    "\n",
    "if damageSpec == 'low':\n",
    "    nordhaus_mean = np.zeros(pers)\n",
    "    nordhaus_std = np.zeros(pers)\n",
    "\n",
    "    for tm in range(pers):\n",
    "        RE_plot[tm] = RE_func(hists[tm,:,0])\n",
    "        weight_plot[tm] = pi_tilde_1_func(hists[tm,:,0])\n",
    "        nordhaus_mean[tm] = beta_tilde_1_func(hists[tm,:,0])\n",
    "        nordhaus_std[tm] = 1 / np.sqrt(lambda_tilde_1_func(hists[tm,:,0]))\n",
    "\n",
    "    REs['RE'] = RE_plot\n",
    "    REs['Weights'] = weight_plot\n",
    "    REs['Shifted Mean'] = nordhaus_mean\n",
    "    REs['Shifted Std'] = nordhaus_std\n",
    "\n",
    "else:\n",
    "    for tm in range(pers):\n",
    "        RE_plot[tm] = RE_func(hists[tm,:,0])\n",
    "        weight_plot[tm] = pi_tilde_1_func(hists[tm,:,0])\n",
    "\n",
    "    REs['RE'] = RE_plot\n",
    "    REs['Weights'] = weight_plot\n",
    "\n",
    "\n",
    "original_dist = norm.pdf(beta_f_space, Œ≤ùòß, np.sqrt(œÉ·µ¶))\n",
    "Dists['Original'] = original_dist\n",
    "    \n",
    "# probabilities (R,K,F)\n",
    "\n",
    "for tm in [1,100,200,300,400]:\n",
    "    R0 = hists[tm-1,0,0]\n",
    "    K0 = hists[tm-1,1,0]\n",
    "    F0 = hists[tm-1,2,0]\n",
    "\n",
    "    # Weitzman\n",
    "    def scale_2_fnc_prob(x):\n",
    "        return np.exp(-1 / Œæp * xi_d * (Œ≥1 * x + Œ≥2 * x ** 2 *  F0 + Œ≥2_plus * x * (x * F0 - FÃÑ) ** (power - 1) * ((x * F0 - FÃÑ) >= 0)) * R0 * e_func([R0, K0, F0])) * norm.pdf(x, Œ≤ùòß, np.sqrt(œÉ·µ¶))\n",
    "    scale_2_prob = quad_int(scale_2_fnc_prob, a, b, n, 'legendre')\n",
    "\n",
    "    q2_tilde_fnc_prob = np.exp(-1 / Œæp * xi_d * (Œ≥1 * beta_f_space + Œ≥2 * beta_f_space ** 2 * F0 + Œ≥2_plus * beta_f_space * (beta_f_space * F0 - FÃÑ) ** (power - 1) * ((beta_f_space * F0 - FÃÑ) >= 0)) * R0* e_func([R0, K0, F0])) / scale_2_prob * norm.pdf(beta_f_space, Œ≤ùòß, np.sqrt(œÉ·µ¶))\n",
    "    weitzman = q2_tilde_fnc_prob\n",
    "\n",
    "    # Nordhaus\n",
    "    mean_distort_nordhaus = beta_tilde_1_func([R0, K0, F0]) - Œ≤ùòß\n",
    "    lambda_tilde_nordhaus = lambda_tilde_1_func([R0, K0, F0])\n",
    "    nordhaus = norm.pdf(beta_f_space, mean_distort_nordhaus + Œ≤ùòß, 1 / np.sqrt(lambda_tilde_nordhaus))\n",
    "\n",
    "    # weights\n",
    "    Dists_weight = pi_tilde_1_func([R0, K0, F0])\n",
    "    if damageSpec == 'High':\n",
    "        Dists['Weitzman_year' + str(int((tm) / 4))] = weitzman\n",
    "    elif damageSpec == 'Low':\n",
    "        Dists['Nordhaus_year' + str(int((tm) / 4))] = nordhaus\n",
    "    elif damageSpec == 'Weighted':\n",
    "        Dists['Weitzman_year' + str(int((tm) / 4))] = weitzman\n",
    "        Dists['Nordhaus_year' + str(int((tm) / 4))] = nordhaus\n",
    "        Dists['Weighted_year' + str(int((tm) / 4))] = nordhaus * Dists_weight + weitzman * (1 - Dists_weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implied worst case densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T22:11:09.492424Z",
     "start_time": "2020-01-02T22:11:08.880334Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting worst case densities\n",
    "densityPlot(beta_f_space, Dists, damageSpec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCC Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T22:11:09.830534Z",
     "start_time": "2020-01-02T22:11:09.494463Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting social cost of carbon decomposition\n",
    "SCCDecomposePlot(SCCs, damageSpec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emission trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T22:11:10.061945Z",
     "start_time": "2020-01-02T22:11:09.831532Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting emissions\n",
    "emissionPlot(damageSpec, Œæp, e_hists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
