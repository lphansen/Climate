#Required packages
import os
import sys
sys.path.append('./src')
from supportfunctions import *
sys.stdout.flush()
import petsc4py
petsc4py.init(sys.argv)
from petsc4py import PETSc
from scipy.sparse import spdiags
from scipy.sparse import coo_matrix
from scipy.sparse import csr_matrix
from datetime import datetime
# Damage function choices
damageSpec = 'Weighted'  # Choose among "High"(Weitzman), 'Low'(Nordhaus) and 'Weighted' (arithmeticAverage of the two)

if damageSpec == 'High':
    weight = 0.0
elif damageSpec == 'Low':
    weight = 1.0
else:
    weight = 0.5

Œæp =  1 / 4000  # Ambiguity Averse Paramter
# We stored solutions for Œæp =  1 / 4000 to which we referred as "Ambiguity Averse" and Œæp = 1000 as ‚ÄúAmbiguity Neutral‚Äù in the paper
# Sensible choices are from 0.0002 to 4000, while for parameters input over 0.01 the final results won't alter as much

if Œæp < 1:
    aversespec = "Averse"
else:
    aversespec = 'Neutral'

smart_guess = False
model = 'petsc'
current_time = datetime.now()
filename = 'BBH_' + model + '_' + damageSpec + '_' + aversespec + '_' + "{}-{}-{}".format(current_time.day, current_time.hour, current_time.minute)

McD = np.loadtxt('./data/TCRE_MacDougallEtAl2017_update.txt')
par_lambda_McD = McD / 1000

Œ≤ùòß = np.mean(par_lambda_McD)  # Climate sensitivity parameter, MacDougall (2017)
œÉ·µ¶ = np.var(par_lambda_McD, ddof = 1)  # varaiance of climate sensitivity parameters
Œª = 1.0 / œÉ·µ¶

quadrature = 'legendre'
n = 30
a = Œ≤ùòß - 5 * np.sqrt(œÉ·µ¶)
b = Œ≤ùòß + 5 * np.sqrt(œÉ·µ¶)

(xs,ws) = quad_points_legendre(n)
xs = (b-a) * 0.5  * xs + (a + b) * 0.5
s = np.prod((b-a) * 0.5)

start_time = time.time()

# Parameters as defined in the paper
Œ¥ = 0.01
Œ∫ = 0.032
œÉùò® = 0.02
œÉùò¨ = 0.0161
œÉùò≥ = 0.0339
Œ± = 0.115000000000000
œï0 = 0.0600
œï1 = 16.666666666666668
Œºk = -0.034977443912449
œà0 = 0.112733407891680
œà1 = 0.142857142857143

# parameters for damage function settings
power = 2
Œ≥1 = 0.00017675
Œ≥2 = 2. * 0.0022
Œ≥2_plus = 2. * 0.0197
Œ≥ÃÑ2_plus = weight * 0 + (1 - weight) * Œ≥2_plus

œÉ1 = 0
œÉ2 = 0
œÅ12 = 0
FÃÑ = 2
crit = 2
F0 = 1

xi_d = -1 * (1 - Œ∫)

# See Remark 2.1.1 regarding the choice of Œµ and Œ∑
# False Trasient Time step
Œµ = 0.3
# Cobweb learning rate
Œ∑ = 0.05


# Specifying Tolerance level
tol = 1e-8

# Grids Specification

# Coarse Grids
# R_min = 0
# R_max = 9
# F_min = 0
# F_max = 4000
# K_min = 0
# K_max = 18
# nR = 4
# nF = 4
# nK = 4
# R = np.linspace(R_min, R_max, nR)
# F = np.linspace(F_min, F_max, nF)
# K = np.linspace(K_min, K_max, nK)

# hR = R[1] - R[0]
# hF = F[1] - F[0]
# hK = K[1] - K[0]

# Dense Grids

R_min = 0
R_max = 9
F_min = 0
F_max = 4000
K_min = 0
K_max = 18

hR = 0.05
hF = 25
hK = 0.15

R = np.arange(R_min, R_max + hR, hR)
nR = len(R)
F = np.arange(F_min, F_max + hF, hF)
nF = len(F)
K = np.arange(K_min, K_max + hK, hK)
nK = len(K)

# Discretization of the state space for numerical PDE solution.
# See Remark 2.1.2
(R_mat, F_mat, K_mat) = np.meshgrid(R,F,K, indexing = 'ij')
stateSpace = np.hstack([R_mat.reshape(-1,1,order = 'F'),F_mat.reshape(-1,1,order = 'F'),K_mat.reshape(-1,1,order = 'F')])

# Inputs for function quad_int
# Integrating across parameter distribution
quadrature = 'legendre'
n = 30
a = Œ≤ùòß - 5 * np.sqrt(œÉ·µ¶)
b = Œ≤ùòß + 5 * np.sqrt(œÉ·µ¶)

v0 = Œ∫ * R_mat + (1-Œ∫) * K_mat - Œ≤ùòß * F_mat

FC_Err = 1
episode = 0

if smart_guess:
    v0 = v0_guess
    q = q_guess
    e_star = e_guess
    episode = 1
    Œµ = 0.2


while FC_Err > tol and episode < 2:
    vold = v0.copy()
    # Applying finite difference scheme to the value function
    v0_dr = finiteDiff(v0,0,1,hR)
    v0_df = finiteDiff(v0,1,1,hF)
    v0_dk = finiteDiff(v0,2,1,hK)

    v0_drr = finiteDiff(v0,0,2,hR)
    v0_drr[v0_dr < 1e-16] = 0
    v0_dr[v0_dr < 1e-16] = 1e-16
    v0_dff = finiteDiff(v0,1,2,hF)
    v0_dkk = finiteDiff(v0,2,2,hK)
    if episode > 2000:
        Œµ = 0.1
    elif episode > 1000:
        Œµ = 0.2
    else:
        pass

    if episode == 0:
        # First time into the loop
        B1 = v0_dr - xi_d * (Œ≥1 + Œ≥2 * F_mat * Œ≤ùòß + Œ≥2_plus * (F_mat * Œ≤ùòß - FÃÑ) ** (power - 1) * (F_mat >= (crit / Œ≤ùòß))) * Œ≤ùòß * np.exp(R_mat) - v0_df * np.exp(R_mat)
        C1 = - Œ¥ * Œ∫
        e = -C1 / B1
        e_hat = e
        Acoeff = np.ones(R_mat.shape)
        Bcoeff = ((Œ¥ * (1 - Œ∫) * œï1 + œï0 * œï1 * v0_dk) * Œ¥ * (1 - Œ∫) / (v0_dr * œà0 * 0.5) * np.exp(0.5 * (R_mat - K_mat))) / (Œ¥ * (1 - Œ∫) * œï1)
        Ccoeff = -Œ±  - 1 / œï1
        j = ((-Bcoeff + np.sqrt(Bcoeff ** 2 - 4 * Acoeff * Ccoeff)) / (2 * Acoeff)) ** 2
        i = Œ± - j - (Œ¥ * (1 - Œ∫)) / (v0_dr * œà0 * 0.5) * j ** 0.5 * np.exp(0.5 * (R_mat - K_mat))
        q = Œ¥ * (1 - Œ∫) / (Œ± - i - j)
    else:
        e_hat = e_star

        # Step 4 (a) : Cobeweb scheme to update controls i and j; q is an intermediary variable that determines i and j
        Converged = 0
        nums = 0
        while Converged == 0:
            i_star = (œï0 * œï1 * v0_dk / q - 1) / œï1
            j_star = (q * np.exp(œà1 * (R_mat - K_mat)) / (v0_dr * œà0 * œà1)) ** (1 / (œà1 - 1))
            if Œ± > np.max(i_star + j_star):
                q_star = Œ∑ * Œ¥ * (1 - Œ∫) / (Œ± - i_star - j_star) + (1 - Œ∑) * q
            else:
                q_star = 2 * q
            if np.max(abs(q - q_star) / Œ∑) <= 1e-5:
                Converged = 1
                q = q_star
                i = i_star
                j = j_star
            else:
                q = q_star
                i = i_star
                j = j_star

            nums += 1
        if episode % 100 == 0:
            print('Cobweb Passed, iterations: {}, i error: {:10f}, j error: {:10f}'.format(nums, np.max(i - i_star), np.max(j - j_star)))

    i[i <= -1/œï1] = - 1/œï1 + 1e-8

    a1 = np.zeros(R_mat.shape)
    b1 = xi_d * e_hat * np.exp(R_mat) * Œ≥1
    c1 = 2 * xi_d * e_hat * np.exp(R_mat) * F_mat * Œ≥2
    ŒªÃÉ1 = Œª + c1 / Œæp
    Œ≤ÃÉ1 = Œ≤ùòß - c1 * Œ≤ùòß / (Œæp * ŒªÃÉ1) -  b1 /  (Œæp * ŒªÃÉ1)
    I1 = a1 - 0.5 * np.log(Œª) * Œæp + 0.5 * np.log(ŒªÃÉ1) * Œæp + 0.5 * Œª * Œ≤ùòß ** 2 * Œæp - 0.5 * ŒªÃÉ1 * (Œ≤ÃÉ1) ** 2 * Œæp
    R1 = 1 / Œæp * (I1 - (a1 + b1 * Œ≤ÃÉ1 + c1 / 2 * Œ≤ÃÉ1 ** 2 + c1 / 2 / ŒªÃÉ1))
    J1_without_e = xi_d * (Œ≥1 * Œ≤ÃÉ1 + Œ≥2 * F_mat * (Œ≤ÃÉ1 ** 2 + 1 / ŒªÃÉ1)) * np.exp(R_mat)

    œÄÃÉ1 = weight * np.exp(-1 / Œæp * I1)

    # Step (2), solve minimization problem in HJB and calculate drift distortion
    # See remark 2.1.3 for more details
    start_time2 = time.time()
    if episode == 0 or (smart_guess and episode == 1):
        #@nb.jit(nopython = True, parallel = True)
        def scale_2_fnc(x, ndist, e_hat):
            return np.exp(-1 / Œæp * x * e_hat) * ndist

        #@nb.jit(nopython = True, parallel = True)
        def q2_tilde_fnc(x, e_hat, scale_2):
            return np.exp(-1 / Œæp * x * e_hat) / scale_2

        #@nb.jit(nopython = True, parallel = True)
        def J2_without_e_fnc(x, ndist, e_hat, scale_2):
            return x * q2_tilde_fnc(x, e_hat, scale_2) * ndist

        (xs,ws) = quad_points_legendre(n)
        xs = (b-a) * 0.5  * xs + (a + b) * 0.5
        s = np.prod((b-a) * 0.5)

        normdists = np.zeros(n)
        distort_terms = np.zeros((n, nR, nF, nK))
        for i_iter in range(n):
            normdists[i_iter] = normpdf(xs[i_iter],Œ≤ùòß,np.sqrt(œÉ·µ¶))
            distort_terms[i_iter] = xi_d * (Œ≥1 * xs[i_iter] + Œ≥2 * xs[i_iter] ** 2 * F_mat + Œ≥2_plus * xs[i_iter] * (xs[i_iter] * F_mat - FÃÑ) ** (power - 1) * ((xs[i_iter] * F_mat - FÃÑ) >= 0)) * np.exp(R_mat)

        dVec = [hR, hF, hK]
        increVec = [1, nR, nR * nF]
        I_LB_R = (stateSpace[:,0] == R_min)
        I_UB_R = (stateSpace[:,0] == R_max)
        I_LB_F = (stateSpace[:,1] == F_min)
        I_UB_F = (stateSpace[:,1] == F_max)
        I_LB_K = (stateSpace[:,2] == K_min)
        I_UB_K = (stateSpace[:,2] == K_max)

    scale_2 = np.zeros(F_mat.shape)
    for i_iter in range(n):
        scale_2 += ws[i_iter] * scale_2_fnc(distort_terms[i_iter], normdists[i_iter], e_hat)
    scale_2 = s * scale_2

    I2 = -1 * Œæp * np.log(scale_2)

    J2_without_e = np.zeros(F_mat.shape)
    for i_iter in range(n):
        J2_without_e += ws[i_iter] * J2_without_e_fnc(distort_terms[i_iter], normdists[i_iter], e_hat, scale_2)
    J2_without_e = s * J2_without_e
    J2_with_e = J2_without_e * e_hat
    end_time2 = time.time()


    R2 = (I2 - J2_with_e) / Œæp
    œÄÃÉ2 = (1 - weight) * np.exp(-1 / Œæp * I2)
    œÄÃÉ1_norm = œÄÃÉ1 / (œÄÃÉ1 + œÄÃÉ2)
    œÄÃÉ2_norm = 1 - œÄÃÉ1_norm

    # step 4 (b) updating e based on first order conditions
    expec_e_sum = (œÄÃÉ1_norm * J1_without_e + œÄÃÉ2_norm * J2_without_e)

    B1 = v0_dr - v0_df * np.exp(R_mat) - expec_e_sum
    C1 = -Œ¥ * Œ∫
    e = -C1 / B1
    e_star = e

    J1 = J1_without_e * e_star
    J2 = J2_without_e * e_star

    # Step (3) calculating implied entropies
    I_term = -1 * Œæp * np.log(œÄÃÉ1 + œÄÃÉ2)

    R1 = (I1 - J1) / Œæp
    R2 = (I2 - J2) / Œæp

    # Step (5) solving for adjusted drift
    drift_distort = (œÄÃÉ1_norm * J1 + œÄÃÉ2_norm * J2)

    if weight == 0 or weight == 1:
        RE = œÄÃÉ1_norm * R1 + œÄÃÉ2_norm * R2
    else:
        RE = œÄÃÉ1_norm * R1 + œÄÃÉ2_norm * R2 + œÄÃÉ1_norm * np.log(
            œÄÃÉ1_norm / weight) + œÄÃÉ2_norm * np.log(œÄÃÉ2_norm / (1 - weight))

    RE_total = Œæp * RE

    # Step (6) and (7) Formulating HJB False Transient parameters
    # See remark 2.1.4 for more details
    A = -Œ¥ * np.ones(R_mat.shape)
    B_r = -e_star + œà0 * (j ** œà1) * np.exp(œà1 * (K_mat - R_mat)) - 0.5 * (œÉùò≥ ** 2)
    B_f = e_star * np.exp(R_mat)
    B_k = Œºk + œï0 * np.log(1 + i * œï1) - 0.5 * (œÉùò¨ ** 2)
    C_rr = 0.5 * œÉùò≥ ** 2 * np.ones(R_mat.shape)
    C_ff = np.zeros(R_mat.shape)

    C_kk = 0.5 * œÉùò¨ ** 2 * np.ones(R_mat.shape)
    D = Œ¥ * Œ∫ * np.log(e_star) + Œ¥ * Œ∫ * R_mat + Œ¥ * (1 - Œ∫) * (np.log(Œ± - i - j) + K_mat) + drift_distort + RE_total # + I_term

    out_cpp = PDESolver(stateSpace, A, B_r, B_f, B_k, C_rr, C_ff, C_kk, D, v0, Œµ, solverType = 'False Transient')
    out_comp_cpp = out_cpp[2].reshape(v0.shape,order = "F")

    PDE_rhs_cpp = A * v0 + B_r * v0_dr + B_f * v0_df + B_k * v0_dk + C_rr * v0_drr + C_kk * v0_dkk + C_ff * v0_dff + D
    PDE_Err_cpp = np.max(abs(PDE_rhs_cpp))
    FC_Err_cpp = np.max(abs((out_comp_cpp - v0)))



    # Transforming the 3-d coefficient matrix to 1-dimensional
    A = A.reshape(-1,1,order = 'F')
    B = np.hstack([B_r.reshape(-1,1,order = 'F'),B_f.reshape(-1,1,order = 'F'),B_k.reshape(-1,1,order = 'F')])
    C = np.hstack([C_rr.reshape(-1,1,order = 'F'), C_ff.reshape(-1,1,order = 'F'), C_kk.reshape(-1,1,order = 'F')])
    D = D.reshape(-1,1,order = 'F')
    v0 = v0.reshape(-1,1,order = 'F')


    # prepare for ksp
    B_plus = np.maximum(B, np.zeros(B.shape))
    B_minus = np.minimum(B, np.zeros(B.shape))

    diag_0 = (A[:,0] - 1 / Œµ
              + (I_LB_R * B[:,0] / -dVec[0] + I_UB_R * B[:,0] / dVec[0] - (1 - I_LB_R - I_UB_R) * (B_plus[:,0] - B_minus[:,0]) / dVec[0] + (I_LB_R * C[:,0] + I_UB_R * C[:,0] - 2 * (1 - I_LB_R - I_UB_R) * C[:,0]) / dVec[0] ** 2)
              + (I_LB_F * B[:,1] / -dVec[1] + I_UB_F * B[:,1] / dVec[1] - (1 - I_LB_F - I_UB_F) * (B_plus[:,1] - B_minus[:,1]) / dVec[1] + (I_LB_F * C[:,1] + I_UB_F * C[:,1] - 2 * (1 - I_LB_F - I_UB_F) * C[:,1]) / dVec[1] ** 2)
              + (I_LB_K * B[:,2] / -dVec[2] + I_UB_K * B[:,2] / dVec[2] - (1 - I_LB_K - I_UB_K) * (B_plus[:,2] - B_minus[:,2]) / dVec[2] + (I_LB_K * C[:,2] + I_UB_K * C[:,2] - 2 * (1 - I_LB_K - I_UB_K) * C[:,2]) / dVec[2] ** 2))
    diag_R = (I_LB_R * B[:,0] / dVec[0] + (1 - I_LB_R - I_UB_R) * B_plus[:,0] / dVec[0] - 2 * I_LB_R * C[:,0] / dVec[0] ** 2 + (1 - I_LB_R - I_UB_R) * C[:,0] / dVec[0] ** 2)
    diag_Rm = (I_UB_R * B[:,0] / -dVec[0] - (1 - I_LB_R - I_UB_R) * B_minus[:,0] / dVec[0] - 2 * I_UB_R * C[:,0] / dVec[0] ** 2 + (1 - I_LB_R - I_UB_R) * C[:,0] / dVec[0] ** 2)
    diag_F = (I_LB_F * B[:,1] / dVec[1] + (1 - I_LB_F - I_UB_F) * B_plus[:,1] / dVec[1] - 2 * I_LB_F * C[:,1] / dVec[1] ** 2 + (1 - I_LB_F - I_UB_F) * C[:,1] / dVec[1] ** 2)
    diag_Fm = (I_UB_F * B[:,1] / -dVec[1] - (1 - I_LB_F - I_UB_F) * B_minus[:,1] / dVec[1] - 2 * I_UB_F * C[:,1] / dVec[1] ** 2 + (1 - I_LB_F - I_UB_F) * C[:,1] / dVec[1] ** 2)
    diag_K = (I_LB_K * B[:,2] / dVec[2] + (1 - I_LB_K - I_UB_K) * B_plus[:,2] / dVec[2] - 2 * I_LB_K * C[:,2] / dVec[2] ** 2 + (1 - I_LB_K - I_UB_K) * C[:,2] / dVec[2] ** 2)
    diag_Km = (I_UB_K * B[:,2] / -dVec[2] - (1 - I_LB_K - I_UB_K) * B_minus[:,2] / dVec[2] - 2 * I_UB_K * C[:,2] / dVec[2] ** 2 + (1 - I_LB_K - I_UB_K) * C[:,2] / dVec[2] ** 2)
    diag_RR = I_LB_R * C[:,0] / dVec[0] ** 2
    diag_RRm = I_UB_R * C[:,0] / dVec[0] ** 2
    diag_FF = I_LB_F * C[:,1] / dVec[1] ** 2
    diag_FFm = I_UB_F * C[:,1] / dVec[1] ** 2
    diag_KK = I_LB_K * C[:,2] / dVec[2] ** 2
    diag_KKm = I_UB_K * C[:,2] / dVec[2] ** 2

    #data = np.vstack([diag_0, diag_R, diag_Rm, diag_RR, diag_RRm, diag_F, diag_Fm, diag_FF, diag_FFm, diag_K, diag_Km, diag_KK, diag_KKm])
    data = [diag_0, diag_R, diag_Rm, diag_RR, diag_RRm, diag_F, diag_Fm, diag_FF, diag_FFm, diag_K, diag_Km, diag_KK, diag_KKm]
    diags = np.array([0,-increVec[0],increVec[0],-2*increVec[0],2*increVec[0],
                     -increVec[1],increVec[1],-2*increVec[1],2*increVec[1],
                     -increVec[2],increVec[2],-2*increVec[2],2*increVec[2]])

    # the transpose of matrix A_sp is the desired A matrix
    A_sp = spdiags(data, diags, len(diag_0), len(diag_0))
    b = -v0 / Œµ - D

    csr_mat = csr_matrix(A_sp.T)
    petsc_mat = PETSc.Mat().createAIJ(size=csr_mat.shape, csr=(csr_mat.indptr, csr_mat.indices, csr_mat.data))
    petsc_rhs = PETSc.Vec().createWithArray(b)
    x = petsc_mat.createVecRight()
    x.set(0)

    # create linear solver
    start = time.time()
    ksp = PETSc.KSP()
    ksp.create(PETSc.COMM_WORLD)
    ksp.setType('lsqr')
    ksp.getPC().setType('none')
    ksp.setOperators(petsc_mat)
    ksp.setFromOptions()
    ksp.setTolerances(rtol=1e-10, atol=1e-20)
    # petsc_time1 = time.time()
    ksp.solve(petsc_rhs, x)
    print(ksp.getConvergedReason())
    out_comp = np.array(ksp.getSolution()).reshape(R_mat.shape,order = "F")
    A = A.reshape(R_mat.shape,order = "F")
    D = D.reshape(R_mat.shape,order = "F")
    v0 = v0.reshape(R_mat.shape,order = "F")
    # Calculating PDE error and False Transient error
    PDE_rhs = A * v0 + B_r * v0_dr + B_f * v0_df + B_k * v0_dk + C_rr * v0_drr + C_kk * v0_dkk + C_ff * v0_dff + D
    PDE_Err = np.max(abs(PDE_rhs))
    FC_Err = np.max(abs((out_comp - v0)))
    if episode % 1 == 0:
        print("Episode {:d}: PDE Error: {:.10f}; False Transient Error: {:.10f};" .format(episode, PDE_Err, FC_Err))
    print("Time for this iteration: {}".format(time.time() - start))

    # print(PDE_Err, PDE_Err_cpp)
    v_cpp = np.array(out_cpp[2])
    res = np.linalg.norm(csr_mat@v_cpp[2] - b )
    np.save("res_cpp", res)
    print(res)
    # step 9: keep iterating until convergence
    v0 = out_comp
    episode += 1

print("Episode {:d}: PDE Error: {:.10f}; False Transient Error: {:.10f}" .format(episode, PDE_Err, FC_Err))
print("--- %s seconds ---" % (time.time() - start_time))

import pickle
# filename = filename
my_shelf = {}
for key in dir():
    if isinstance(globals()[key], (int,float, np.float, str, bool, np.ndarray,list)):
        try:
            my_shelf[key] = globals()[key]
        except TypeError:
            #
            # __builtins__, my_shelf, and imported modules can not be shelved.
            #
            print('ERROR shelving: {0}'.format(key))
    else:
        pass


file = open(filename, 'wb')
pickle.dump(my_shelf, file)
file.close()
